{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習\n",
    "\n",
    "ステップ１で作った関数をもう一度実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# numpyを導入\n",
    "import numpy as np\n",
    "\n",
    "# モデル作成\n",
    "def create_model():    \n",
    "    model = {\n",
    "        # 荷重を -5 ~ 5 の乱数で初期化\n",
    "        \"weights\" : np.random.uniform(-5, 5, 2),    \n",
    "        # バイアスも！\n",
    "        \"bias\"    : np.random.uniform(-5, 5, 1)}\n",
    "    return model\n",
    "\n",
    "# 推論\n",
    "def predict(model, activation, x):\n",
    "    \n",
    "    # 足し算を計算し…\n",
    "    y = model[\"weights\"][0] * x[0] + model[\"weights\"][1] * x[1] + model[\"bias\"]\n",
    "    \n",
    "    # 活性化で処理し、その結果を返す\n",
    "    y = activation(y)\n",
    "    return y\n",
    "\n",
    "# 線形活性化関数\n",
    "def linear_activation(x):\n",
    "    return x\n",
    "\n",
    "# ステップ活性化関数\n",
    "def step_activation(x):\n",
    "    if x >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差を計算\n",
    "\n",
    "正しい答え（ラベル）と推論した答えの差分を用い、学習させる。\n",
    "\n",
    "まず、「AND」のラベルを準備しよう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 入力\n",
    "x_list = np.array([\n",
    "    [0, 0], \n",
    "    [0, 1], \n",
    "    [1, 0], \n",
    "    [1, 1]\n",
    "], dtype = float)\n",
    "print(x_list.shape)\n",
    "\n",
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [0], \n",
    "    [0], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差（損失）関数を実装しよう。\n",
    "\n",
    "課題により、適切な関数を使うべきが、今回の入門課題はただの「差分」にしよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "def error(y_true, y_pred):\n",
    "    return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示する\n",
    "def print_results(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "\n",
    "    for i in range(data_size):\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        err = error(y_t, y_p)\n",
    "        print(x, y_t, \"->\", y_p, \"err:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [3.98567413] err: [-3.98567413]\n",
      "[0. 1.] [0.] -> [0.73090718] err: [-0.73090718]\n",
      "[1. 0.] [0.] -> [8.11939151] err: [-8.11939151]\n",
      "[1. 1.] [1.] -> [4.86462457] err: [-3.86462457]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 1 err: [-1.]\n",
      "[0. 1.] [0.] -> 1 err: [-1.]\n",
      "[1. 0.] [0.] -> 1 err: [-1.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "誤差にて、荷重を調整しよう。ただ、「入力」は「０」であると、出力に影響がないため、入力は「１」のときだけに荷重を調整する、つまり：\n",
    "\n",
    "$$ w_i' = w_i + x_i \\cdot error(y_{true}, y_{pred}) $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 荷重を更新する関数\n",
    "def update_weight(w, x, err):\n",
    "    return w + x * err\n",
    "\n",
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "\n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "\n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "\n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "\n",
    "        # 誤差を計算\n",
    "        err  = error(y_t, y_p)\n",
    "        mse += err * err\n",
    "\n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "\n",
    "        w0   = update_weight(w0, x[0], err)\n",
    "        w1   = update_weight(w1, x[1], err)\n",
    "        bias = update_weight(bias, 1 , err)\n",
    "\n",
    "        model[\"weights\"][0] = w0[0]\n",
    "        model[\"weights\"][1] = w1[0]\n",
    "        model[\"bias\"] = bias[0]\n",
    "\n",
    "    #誤差（損失）としては、平均値を返す\n",
    "    return mse / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [37.85886901]\n"
     ]
    }
   ],
   "source": [
    "loss = fit_single_step(model, linear_activation, x_list, y_true)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 4.254766942751925 err: [-4.25476694]\n",
      "[0. 1.] [0.] -> 12.643251269366607 err: [-12.64325127]\n",
      "[1. 0.] [0.] -> 9.388484326614682 err: [-9.38848433]\n",
      "[1. 1.] [1.] -> 17.776968653229364 err: [-16.77696865]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, activation, x_list, y_true, epochs):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss = fit_single_step(model, activation, x_list, y_true)\n",
    "        print(\"epoch:\", i, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [26.03679821]\n",
      "epoch: 1 loss: [54.54599546]\n",
      "epoch: 2 loss: [26.34888996]\n",
      "epoch: 3 loss: [67.00133848]\n",
      "epoch: 4 loss: [28.66098171]\n",
      "epoch: 5 loss: [81.4566815]\n",
      "epoch: 6 loss: [32.97307346]\n",
      "epoch: 7 loss: [97.91202452]\n",
      "epoch: 8 loss: [39.28516521]\n",
      "epoch: 9 loss: [116.36736754]\n",
      "epoch: 10 loss: [47.59725696]\n",
      "epoch: 11 loss: [136.82271056]\n",
      "epoch: 12 loss: [57.90934871]\n",
      "epoch: 13 loss: [159.27805358]\n",
      "epoch: 14 loss: [70.22144046]\n",
      "epoch: 15 loss: [183.73339659]\n",
      "epoch: 16 loss: [84.53353221]\n",
      "epoch: 17 loss: [210.18873961]\n",
      "epoch: 18 loss: [100.84562396]\n",
      "epoch: 19 loss: [238.64408263]\n",
      "epoch: 20 loss: [119.1577157]\n",
      "epoch: 21 loss: [269.09942565]\n",
      "epoch: 22 loss: [139.46980745]\n",
      "epoch: 23 loss: [301.55476867]\n",
      "epoch: 24 loss: [161.7818992]\n",
      "epoch: 25 loss: [336.01011169]\n",
      "epoch: 26 loss: [186.09399095]\n",
      "epoch: 27 loss: [372.46545471]\n",
      "epoch: 28 loss: [212.4060827]\n",
      "epoch: 29 loss: [410.92079772]\n",
      "epoch: 30 loss: [240.71817445]\n",
      "epoch: 31 loss: [451.37614074]\n",
      "epoch: 32 loss: [271.0302662]\n",
      "epoch: 33 loss: [493.83148376]\n",
      "epoch: 34 loss: [303.34235795]\n",
      "epoch: 35 loss: [538.28682678]\n",
      "epoch: 36 loss: [337.6544497]\n",
      "epoch: 37 loss: [584.7421698]\n",
      "epoch: 38 loss: [373.96654145]\n",
      "epoch: 39 loss: [633.19751282]\n",
      "epoch: 40 loss: [412.2786332]\n",
      "epoch: 41 loss: [683.65285584]\n",
      "epoch: 42 loss: [452.59072495]\n",
      "epoch: 43 loss: [736.10819885]\n",
      "epoch: 44 loss: [494.9028167]\n",
      "epoch: 45 loss: [790.56354187]\n",
      "epoch: 46 loss: [539.21490844]\n",
      "epoch: 47 loss: [847.01888489]\n",
      "epoch: 48 loss: [585.52700019]\n",
      "epoch: 49 loss: [905.47422791]\n",
      "epoch: 50 loss: [633.83909194]\n",
      "epoch: 51 loss: [965.92957093]\n",
      "epoch: 52 loss: [684.15118369]\n",
      "epoch: 53 loss: [1028.38491395]\n",
      "epoch: 54 loss: [736.46327544]\n",
      "epoch: 55 loss: [1092.84025697]\n",
      "epoch: 56 loss: [790.77536719]\n",
      "epoch: 57 loss: [1159.29559998]\n",
      "epoch: 58 loss: [847.08745894]\n",
      "epoch: 59 loss: [1227.750943]\n",
      "epoch: 60 loss: [905.39955069]\n",
      "epoch: 61 loss: [1298.20628602]\n",
      "epoch: 62 loss: [965.71164244]\n",
      "epoch: 63 loss: [1370.66162904]\n",
      "epoch: 64 loss: [1028.02373419]\n",
      "epoch: 65 loss: [1445.11697206]\n",
      "epoch: 66 loss: [1092.33582594]\n",
      "epoch: 67 loss: [1521.57231508]\n",
      "epoch: 68 loss: [1158.64791769]\n",
      "epoch: 69 loss: [1600.0276581]\n",
      "epoch: 70 loss: [1226.96000943]\n",
      "epoch: 71 loss: [1680.48300111]\n",
      "epoch: 72 loss: [1297.27210118]\n",
      "epoch: 73 loss: [1762.93834413]\n",
      "epoch: 74 loss: [1369.58419293]\n",
      "epoch: 75 loss: [1847.39368715]\n",
      "epoch: 76 loss: [1443.89628468]\n",
      "epoch: 77 loss: [1933.84903017]\n",
      "epoch: 78 loss: [1520.20837643]\n",
      "epoch: 79 loss: [2022.30437319]\n",
      "epoch: 80 loss: [1598.52046818]\n",
      "epoch: 81 loss: [2112.75971621]\n",
      "epoch: 82 loss: [1678.83255993]\n",
      "epoch: 83 loss: [2205.21505923]\n",
      "epoch: 84 loss: [1761.14465168]\n",
      "epoch: 85 loss: [2299.67040224]\n",
      "epoch: 86 loss: [1845.45674343]\n",
      "epoch: 87 loss: [2396.12574526]\n",
      "epoch: 88 loss: [1931.76883518]\n",
      "epoch: 89 loss: [2494.58108828]\n",
      "epoch: 90 loss: [2020.08092693]\n",
      "epoch: 91 loss: [2595.0364313]\n",
      "epoch: 92 loss: [2110.39301868]\n",
      "epoch: 93 loss: [2697.49177432]\n",
      "epoch: 94 loss: [2202.70511042]\n",
      "epoch: 95 loss: [2801.94711734]\n",
      "epoch: 96 loss: [2297.01720217]\n",
      "epoch: 97 loss: [2908.40246036]\n",
      "epoch: 98 loss: [2393.32929392]\n",
      "epoch: 99 loss: [3016.85780337]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習率とは\n",
    "\n",
    "上記のように、差分だけを直そうとすると、平均的な誤差がお大きくなってしまう。その理由は、確か、勾配の方向が正しいが、ステップが大きいすぎる。つまり、最適な数値から大幅に超えてしまい、段々離れてしまう。\n",
    "\n",
    "「学習率」という係数で、ステップの大きさを小さくし、少しずつ最適な数値に近づくようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "\n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "\n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "\n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "\n",
    "        # 誤差を計算\n",
    "        err = error(y_t, y_p)\n",
    "        mse = err * err\n",
    "\n",
    "        # 学習率\n",
    "        learning_rate = 0.01\n",
    "\n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "\n",
    "        w0   = update_weight(w0, x[0], err * learning_rate)\n",
    "        w1   = update_weight(w1, x[1], err * learning_rate)\n",
    "        bias = update_weight(bias, 1 , err * learning_rate)\n",
    "\n",
    "        model[\"weights\"][0] = w0[0]\n",
    "        model[\"weights\"][1] = w1[0]\n",
    "        model[\"bias\"] = bias[0]\n",
    "\n",
    "    return mse / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [3351.1425915]\n",
      "epoch: 1 loss: [3104.55633317]\n",
      "epoch: 2 loss: [2880.94456366]\n",
      "epoch: 3 loss: [2677.94607308]\n",
      "epoch: 4 loss: [2493.45736193]\n",
      "epoch: 5 loss: [2325.60336468]\n",
      "epoch: 6 loss: [2172.71158543]\n",
      "epoch: 7 loss: [2033.28924186]\n",
      "epoch: 8 loss: [1906.00306165]\n",
      "epoch: 9 loss: [1789.6614182]\n",
      "epoch: 10 loss: [1683.19852984]\n",
      "epoch: 11 loss: [1585.66047942]\n",
      "epoch: 12 loss: [1496.19284044]\n",
      "epoch: 13 loss: [1414.02972104]\n",
      "epoch: 14 loss: [1338.48405973]\n",
      "epoch: 15 loss: [1268.93902647]\n",
      "epoch: 16 loss: [1204.84040007]\n",
      "epoch: 17 loss: [1145.68980813]\n",
      "epoch: 18 loss: [1091.03872928]\n",
      "epoch: 19 loss: [1040.48316927]\n",
      "epoch: 20 loss: [993.65893286]\n",
      "epoch: 21 loss: [950.23742286]\n",
      "epoch: 22 loss: [909.92190552]\n",
      "epoch: 23 loss: [872.44418864]\n",
      "epoch: 24 loss: [837.56166533]\n",
      "epoch: 25 loss: [805.05468147]\n",
      "epoch: 26 loss: [774.72419015]\n",
      "epoch: 27 loss: [746.38966045]\n",
      "epoch: 28 loss: [719.88721185]\n",
      "epoch: 29 loss: [695.06794877]\n",
      "epoch: 30 loss: [671.79647289]\n",
      "epoch: 31 loss: [649.94955325]\n",
      "epoch: 32 loss: [629.41493661]\n",
      "epoch: 33 loss: [610.09028248]\n",
      "epoch: 34 loss: [591.88220914]\n",
      "epoch: 35 loss: [574.7054383]\n",
      "epoch: 36 loss: [558.48202774]\n",
      "epoch: 37 loss: [543.14068225]\n",
      "epoch: 38 loss: [528.61613447]\n",
      "epoch: 39 loss: [514.84858803]\n",
      "epoch: 40 loss: [501.78321635]\n",
      "epoch: 41 loss: [489.36971116]\n",
      "epoch: 42 loss: [477.56187543]\n",
      "epoch: 43 loss: [466.31725611]\n",
      "epoch: 44 loss: [455.59681246]\n",
      "epoch: 45 loss: [445.36461624]\n",
      "epoch: 46 loss: [435.58758056]\n",
      "epoch: 47 loss: [426.23521432]\n",
      "epoch: 48 loss: [417.27939978]\n",
      "epoch: 49 loss: [408.69419076]\n",
      "epoch: 50 loss: [400.45562953]\n",
      "epoch: 51 loss: [392.54158045]\n",
      "epoch: 52 loss: [384.93157869]\n",
      "epoch: 53 loss: [377.60669265]\n",
      "epoch: 54 loss: [370.54939857]\n",
      "epoch: 55 loss: [363.74346633]\n",
      "epoch: 56 loss: [357.17385527]\n",
      "epoch: 57 loss: [350.82661904]\n",
      "epoch: 58 loss: [344.68881872]\n",
      "epoch: 59 loss: [338.74844341]\n",
      "epoch: 60 loss: [332.99433753]\n",
      "epoch: 61 loss: [327.41613427]\n",
      "epoch: 62 loss: [322.00419463]\n",
      "epoch: 63 loss: [316.74955152]\n",
      "epoch: 64 loss: [311.64385846]\n",
      "epoch: 65 loss: [306.67934253]\n",
      "epoch: 66 loss: [301.84876107]\n",
      "epoch: 67 loss: [297.14536192]\n",
      "epoch: 68 loss: [292.56284688]\n",
      "epoch: 69 loss: [288.095338]\n",
      "epoch: 70 loss: [283.73734661]\n",
      "epoch: 71 loss: [279.48374478]\n",
      "epoch: 72 loss: [275.32973892]\n",
      "epoch: 73 loss: [271.27084557]\n",
      "epoch: 74 loss: [267.30286894]\n",
      "epoch: 75 loss: [263.42188025]\n",
      "epoch: 76 loss: [259.6241986]\n",
      "epoch: 77 loss: [255.90637333]\n",
      "epoch: 78 loss: [252.26516766]\n",
      "epoch: 79 loss: [248.69754363]\n",
      "epoch: 80 loss: [245.2006481]\n",
      "epoch: 81 loss: [241.77179985]\n",
      "epoch: 82 loss: [238.40847757]\n",
      "epoch: 83 loss: [235.1083088]\n",
      "epoch: 84 loss: [231.86905964]\n",
      "epoch: 85 loss: [228.68862523]\n",
      "epoch: 86 loss: [225.56502092]\n",
      "epoch: 87 loss: [222.4963741]\n",
      "epoch: 88 loss: [219.48091654]\n",
      "epoch: 89 loss: [216.51697743]\n",
      "epoch: 90 loss: [213.60297676]\n",
      "epoch: 91 loss: [210.73741928]\n",
      "epoch: 92 loss: [207.91888885]\n",
      "epoch: 93 loss: [205.14604319]\n",
      "epoch: 94 loss: [202.41760901]\n",
      "epoch: 95 loss: [199.73237752]\n",
      "epoch: 96 loss: [197.08920014]\n",
      "epoch: 97 loss: [194.48698469]\n",
      "epoch: 98 loss: [191.92469164]\n",
      "epoch: 99 loss: [189.40133084]\n",
      "epoch: 100 loss: [186.91595827]\n",
      "epoch: 101 loss: [184.46767318]\n",
      "epoch: 102 loss: [182.05561531]\n",
      "epoch: 103 loss: [179.67896239]\n",
      "epoch: 104 loss: [177.33692775]\n",
      "epoch: 105 loss: [175.02875814]\n",
      "epoch: 106 loss: [172.75373166]\n",
      "epoch: 107 loss: [170.51115585]\n",
      "epoch: 108 loss: [168.30036591]\n",
      "epoch: 109 loss: [166.12072305]\n",
      "epoch: 110 loss: [163.97161295]\n",
      "epoch: 111 loss: [161.85244427]\n",
      "epoch: 112 loss: [159.76264736]\n",
      "epoch: 113 loss: [157.70167298]\n",
      "epoch: 114 loss: [155.66899113]\n",
      "epoch: 115 loss: [153.66408995]\n",
      "epoch: 116 loss: [151.68647472]\n",
      "epoch: 117 loss: [149.73566689]\n",
      "epoch: 118 loss: [147.81120318]\n",
      "epoch: 119 loss: [145.91263481]\n",
      "epoch: 120 loss: [144.03952663]\n",
      "epoch: 121 loss: [142.19145649]\n",
      "epoch: 122 loss: [140.36801447]\n",
      "epoch: 123 loss: [138.56880233]\n",
      "epoch: 124 loss: [136.79343288]\n",
      "epoch: 125 loss: [135.0415294]\n",
      "epoch: 126 loss: [133.31272518]\n",
      "epoch: 127 loss: [131.60666298]\n",
      "epoch: 128 loss: [129.92299461]\n",
      "epoch: 129 loss: [128.26138048]\n",
      "epoch: 130 loss: [126.62148924]\n",
      "epoch: 131 loss: [125.00299735]\n",
      "epoch: 132 loss: [123.40558877]\n",
      "epoch: 133 loss: [121.82895462]\n",
      "epoch: 134 loss: [120.27279288]\n",
      "epoch: 135 loss: [118.73680808]\n",
      "epoch: 136 loss: [117.22071105]\n",
      "epoch: 137 loss: [115.72421864]\n",
      "epoch: 138 loss: [114.24705351]\n",
      "epoch: 139 loss: [112.78894387]\n",
      "epoch: 140 loss: [111.34962331]\n",
      "epoch: 141 loss: [109.92883055]\n",
      "epoch: 142 loss: [108.5263093]\n",
      "epoch: 143 loss: [107.14180805]\n",
      "epoch: 144 loss: [105.77507989]\n",
      "epoch: 145 loss: [104.42588242]\n",
      "epoch: 146 loss: [103.09397751]\n",
      "epoch: 147 loss: [101.77913123]\n",
      "epoch: 148 loss: [100.48111368]\n",
      "epoch: 149 loss: [99.19969888]\n",
      "epoch: 150 loss: [97.93466464]\n",
      "epoch: 151 loss: [96.68579245]\n",
      "epoch: 152 loss: [95.45286739]\n",
      "epoch: 153 loss: [94.235678]\n",
      "epoch: 154 loss: [93.0340162]\n",
      "epoch: 155 loss: [91.8476772]\n",
      "epoch: 156 loss: [90.6764594]\n",
      "epoch: 157 loss: [89.52016432]\n",
      "epoch: 158 loss: [88.37859653]\n",
      "epoch: 159 loss: [87.25156353]\n",
      "epoch: 160 loss: [86.13887575]\n",
      "epoch: 161 loss: [85.04034641]\n",
      "epoch: 162 loss: [83.9557915]\n",
      "epoch: 163 loss: [82.88502971]\n",
      "epoch: 164 loss: [81.82788234]\n",
      "epoch: 165 loss: [80.78417329]\n",
      "epoch: 166 loss: [79.75372898]\n",
      "epoch: 167 loss: [78.73637829]\n",
      "epoch: 168 loss: [77.73195253]\n",
      "epoch: 169 loss: [76.74028536]\n",
      "epoch: 170 loss: [75.76121278]\n",
      "epoch: 171 loss: [74.79457307]\n",
      "epoch: 172 loss: [73.84020673]\n",
      "epoch: 173 loss: [72.89795647]\n",
      "epoch: 174 loss: [71.96766715]\n",
      "epoch: 175 loss: [71.04918573]\n",
      "epoch: 176 loss: [70.14236125]\n",
      "epoch: 177 loss: [69.24704482]\n",
      "epoch: 178 loss: [68.36308951]\n",
      "epoch: 179 loss: [67.49035038]\n",
      "epoch: 180 loss: [66.62868444]\n",
      "epoch: 181 loss: [65.77795059]\n",
      "epoch: 182 loss: [64.9380096]\n",
      "epoch: 183 loss: [64.10872411]\n",
      "epoch: 184 loss: [63.28995853]\n",
      "epoch: 185 loss: [62.4815791]\n",
      "epoch: 186 loss: [61.6834538]\n",
      "epoch: 187 loss: [60.89545233]\n",
      "epoch: 188 loss: [60.1174461]\n",
      "epoch: 189 loss: [59.34930822]\n",
      "epoch: 190 loss: [58.59091343]\n",
      "epoch: 191 loss: [57.84213809]\n",
      "epoch: 192 loss: [57.1028602]\n",
      "epoch: 193 loss: [56.3729593]\n",
      "epoch: 194 loss: [55.65231651]\n",
      "epoch: 195 loss: [54.9408145]\n",
      "epoch: 196 loss: [54.23833742]\n",
      "epoch: 197 loss: [53.54477094]\n",
      "epoch: 198 loss: [52.86000218]\n",
      "epoch: 199 loss: [52.18391973]\n",
      "epoch: 200 loss: [51.51641358]\n",
      "epoch: 201 loss: [50.85737518]\n",
      "epoch: 202 loss: [50.20669732]\n",
      "epoch: 203 loss: [49.56427419]\n",
      "epoch: 204 loss: [48.93000132]\n",
      "epoch: 205 loss: [48.30377559]\n",
      "epoch: 206 loss: [47.68549517]\n",
      "epoch: 207 loss: [47.07505956]\n",
      "epoch: 208 loss: [46.47236953]\n",
      "epoch: 209 loss: [45.87732709]\n",
      "epoch: 210 loss: [45.28983553]\n",
      "epoch: 211 loss: [44.70979936]\n",
      "epoch: 212 loss: [44.13712429]\n",
      "epoch: 213 loss: [43.57171725]\n",
      "epoch: 214 loss: [43.01348634]\n",
      "epoch: 215 loss: [42.46234082]\n",
      "epoch: 216 loss: [41.91819112]\n",
      "epoch: 217 loss: [41.38094879]\n",
      "epoch: 218 loss: [40.85052651]\n",
      "epoch: 219 loss: [40.32683806]\n",
      "epoch: 220 loss: [39.80979832]\n",
      "epoch: 221 loss: [39.29932324]\n",
      "epoch: 222 loss: [38.79532985]\n",
      "epoch: 223 loss: [38.29773621]\n",
      "epoch: 224 loss: [37.80646142]\n",
      "epoch: 225 loss: [37.32142563]\n",
      "epoch: 226 loss: [36.84254996]\n",
      "epoch: 227 loss: [36.36975656]\n",
      "epoch: 228 loss: [35.90296854]\n",
      "epoch: 229 loss: [35.44211001]\n",
      "epoch: 230 loss: [34.98710601]\n",
      "epoch: 231 loss: [34.53788253]\n",
      "epoch: 232 loss: [34.09436653]\n",
      "epoch: 233 loss: [33.65648586]\n",
      "epoch: 234 loss: [33.22416927]\n",
      "epoch: 235 loss: [32.79734645]\n",
      "epoch: 236 loss: [32.37594795]\n",
      "epoch: 237 loss: [31.95990521]\n",
      "epoch: 238 loss: [31.54915052]\n",
      "epoch: 239 loss: [31.14361703]\n",
      "epoch: 240 loss: [30.74323875]\n",
      "epoch: 241 loss: [30.34795051]\n",
      "epoch: 242 loss: [29.95768795]\n",
      "epoch: 243 loss: [29.57238754]\n",
      "epoch: 244 loss: [29.19198656]\n",
      "epoch: 245 loss: [28.81642306]\n",
      "epoch: 246 loss: [28.44563587]\n",
      "epoch: 247 loss: [28.07956462]\n",
      "epoch: 248 loss: [27.71814967]\n",
      "epoch: 249 loss: [27.36133214]\n",
      "epoch: 250 loss: [27.00905392]\n",
      "epoch: 251 loss: [26.6612576]\n",
      "epoch: 252 loss: [26.3178865]\n",
      "epoch: 253 loss: [25.97888466]\n",
      "epoch: 254 loss: [25.64419684]\n",
      "epoch: 255 loss: [25.31376847]\n",
      "epoch: 256 loss: [24.98754569]\n",
      "epoch: 257 loss: [24.6654753]\n",
      "epoch: 258 loss: [24.34750479]\n",
      "epoch: 259 loss: [24.03358231]\n",
      "epoch: 260 loss: [23.72365664]\n",
      "epoch: 261 loss: [23.41767724]\n",
      "epoch: 262 loss: [23.11559418]\n",
      "epoch: 263 loss: [22.81735818]\n",
      "epoch: 264 loss: [22.52292056]\n",
      "epoch: 265 loss: [22.23223328]\n",
      "epoch: 266 loss: [21.94524888]\n",
      "epoch: 267 loss: [21.66192052]\n",
      "epoch: 268 loss: [21.38220194]\n",
      "epoch: 269 loss: [21.10604745]\n",
      "epoch: 270 loss: [20.83341197]\n",
      "epoch: 271 loss: [20.56425097]\n",
      "epoch: 272 loss: [20.29852046]\n",
      "epoch: 273 loss: [20.03617705]\n",
      "epoch: 274 loss: [19.77717785]\n",
      "epoch: 275 loss: [19.52148056]\n",
      "epoch: 276 loss: [19.26904336]\n",
      "epoch: 277 loss: [19.01982501]\n",
      "epoch: 278 loss: [18.77378474]\n",
      "epoch: 279 loss: [18.53088234]\n",
      "epoch: 280 loss: [18.29107808]\n",
      "epoch: 281 loss: [18.05433273]\n",
      "epoch: 282 loss: [17.82060757]\n",
      "epoch: 283 loss: [17.58986436]\n",
      "epoch: 284 loss: [17.36206535]\n",
      "epoch: 285 loss: [17.13717324]\n",
      "epoch: 286 loss: [16.91515124]\n",
      "epoch: 287 loss: [16.69596299]\n",
      "epoch: 288 loss: [16.4795726]\n",
      "epoch: 289 loss: [16.26594465]\n",
      "epoch: 290 loss: [16.05504415]\n",
      "epoch: 291 loss: [15.84683654]\n",
      "epoch: 292 loss: [15.64128772]\n",
      "epoch: 293 loss: [15.43836401]\n",
      "epoch: 294 loss: [15.23803215]\n",
      "epoch: 295 loss: [15.04025931]\n",
      "epoch: 296 loss: [14.84501306]\n",
      "epoch: 297 loss: [14.65226139]\n",
      "epoch: 298 loss: [14.4619727]\n",
      "epoch: 299 loss: [14.27411577]\n",
      "epoch: 300 loss: [14.08865979]\n",
      "epoch: 301 loss: [13.90557433]\n",
      "epoch: 302 loss: [13.72482935]\n",
      "epoch: 303 loss: [13.54639518]\n",
      "epoch: 304 loss: [13.37024254]\n",
      "epoch: 305 loss: [13.1963425]\n",
      "epoch: 306 loss: [13.02466652]\n",
      "epoch: 307 loss: [12.85518638]\n",
      "epoch: 308 loss: [12.68787427]\n",
      "epoch: 309 loss: [12.52270268]\n",
      "epoch: 310 loss: [12.35964448]\n",
      "epoch: 311 loss: [12.19867287]\n",
      "epoch: 312 loss: [12.0397614]\n",
      "epoch: 313 loss: [11.88288392]\n",
      "epoch: 314 loss: [11.72801466]\n",
      "epoch: 315 loss: [11.57512814]\n",
      "epoch: 316 loss: [11.42419921]\n",
      "epoch: 317 loss: [11.27520304]\n",
      "epoch: 318 loss: [11.1281151]\n",
      "epoch: 319 loss: [10.98291121]\n",
      "epoch: 320 loss: [10.83956744]\n",
      "epoch: 321 loss: [10.69806021]\n",
      "epoch: 322 loss: [10.5583662]\n",
      "epoch: 323 loss: [10.42046241]\n",
      "epoch: 324 loss: [10.28432612]\n",
      "epoch: 325 loss: [10.1499349]\n",
      "epoch: 326 loss: [10.01726661]\n",
      "epoch: 327 loss: [9.88629937]\n",
      "epoch: 328 loss: [9.75701159]\n",
      "epoch: 329 loss: [9.62938195]\n",
      "epoch: 330 loss: [9.50338941]\n",
      "epoch: 331 loss: [9.37901317]\n",
      "epoch: 332 loss: [9.25623272]\n",
      "epoch: 333 loss: [9.1350278]\n",
      "epoch: 334 loss: [9.01537839]\n",
      "epoch: 335 loss: [8.89726475]\n",
      "epoch: 336 loss: [8.78066737]\n",
      "epoch: 337 loss: [8.66556699]\n",
      "epoch: 338 loss: [8.55194459]\n",
      "epoch: 339 loss: [8.43978141]\n",
      "epoch: 340 loss: [8.32905891]\n",
      "epoch: 341 loss: [8.21975878]\n",
      "epoch: 342 loss: [8.11186295]\n",
      "epoch: 343 loss: [8.00535358]\n",
      "epoch: 344 loss: [7.90021305]\n",
      "epoch: 345 loss: [7.79642396]\n",
      "epoch: 346 loss: [7.69396914]\n",
      "epoch: 347 loss: [7.59283163]\n",
      "epoch: 348 loss: [7.49299469]\n",
      "epoch: 349 loss: [7.39444179]\n",
      "epoch: 350 loss: [7.29715659]\n",
      "epoch: 351 loss: [7.20112299]\n",
      "epoch: 352 loss: [7.10632507]\n",
      "epoch: 353 loss: [7.01274712]\n",
      "epoch: 354 loss: [6.92037362]\n",
      "epoch: 355 loss: [6.82918925]\n",
      "epoch: 356 loss: [6.7391789]\n",
      "epoch: 357 loss: [6.65032763]\n",
      "epoch: 358 loss: [6.5626207]\n",
      "epoch: 359 loss: [6.47604354]\n",
      "epoch: 360 loss: [6.39058179]\n",
      "epoch: 361 loss: [6.30622125]\n",
      "epoch: 362 loss: [6.22294791]\n",
      "epoch: 363 loss: [6.14074793]\n",
      "epoch: 364 loss: [6.05960767]\n",
      "epoch: 365 loss: [5.97951362]\n",
      "epoch: 366 loss: [5.90045247]\n",
      "epoch: 367 loss: [5.82241107]\n",
      "epoch: 368 loss: [5.74537644]\n",
      "epoch: 369 loss: [5.66933577]\n",
      "epoch: 370 loss: [5.59427639]\n",
      "epoch: 371 loss: [5.52018582]\n",
      "epoch: 372 loss: [5.4470517]\n",
      "epoch: 373 loss: [5.37486187]\n",
      "epoch: 374 loss: [5.3036043]\n",
      "epoch: 375 loss: [5.2332671]\n",
      "epoch: 376 loss: [5.16383856]\n",
      "epoch: 377 loss: [5.09530709]\n",
      "epoch: 378 loss: [5.02766127]\n",
      "epoch: 379 loss: [4.96088981]\n",
      "epoch: 380 loss: [4.89498158]\n",
      "epoch: 381 loss: [4.82992555]\n",
      "epoch: 382 loss: [4.76571089]\n",
      "epoch: 383 loss: [4.70232685]\n",
      "epoch: 384 loss: [4.63976285]\n",
      "epoch: 385 loss: [4.57800844]\n",
      "epoch: 386 loss: [4.51705329]\n",
      "epoch: 387 loss: [4.45688722]\n",
      "epoch: 388 loss: [4.39750014]\n",
      "epoch: 389 loss: [4.33888214]\n",
      "epoch: 390 loss: [4.2810234]\n",
      "epoch: 391 loss: [4.22391424]\n",
      "epoch: 392 loss: [4.16754509]\n",
      "epoch: 393 loss: [4.1119065]\n",
      "epoch: 394 loss: [4.05698917]\n",
      "epoch: 395 loss: [4.00278388]\n",
      "epoch: 396 loss: [3.94928154]\n",
      "epoch: 397 loss: [3.89647319]\n",
      "epoch: 398 loss: [3.84434995]\n",
      "epoch: 399 loss: [3.7929031]\n",
      "epoch: 400 loss: [3.74212398]\n",
      "epoch: 401 loss: [3.69200408]\n",
      "epoch: 402 loss: [3.64253498]\n",
      "epoch: 403 loss: [3.59370835]\n",
      "epoch: 404 loss: [3.545516]\n",
      "epoch: 405 loss: [3.49794983]\n",
      "epoch: 406 loss: [3.45100183]\n",
      "epoch: 407 loss: [3.4046641]\n",
      "epoch: 408 loss: [3.35892885]\n",
      "epoch: 409 loss: [3.31378837]\n",
      "epoch: 410 loss: [3.26923507]\n",
      "epoch: 411 loss: [3.22526143]\n",
      "epoch: 412 loss: [3.18186005]\n",
      "epoch: 413 loss: [3.13902361]\n",
      "epoch: 414 loss: [3.09674489]\n",
      "epoch: 415 loss: [3.05501675]\n",
      "epoch: 416 loss: [3.01383215]\n",
      "epoch: 417 loss: [2.97318414]\n",
      "epoch: 418 loss: [2.93306586]\n",
      "epoch: 419 loss: [2.89347052]\n",
      "epoch: 420 loss: [2.85439144]\n",
      "epoch: 421 loss: [2.815822]\n",
      "epoch: 422 loss: [2.77775569]\n",
      "epoch: 423 loss: [2.74018606]\n",
      "epoch: 424 loss: [2.70310676]\n",
      "epoch: 425 loss: [2.66651151]\n",
      "epoch: 426 loss: [2.63039411]\n",
      "epoch: 427 loss: [2.59474844]\n",
      "epoch: 428 loss: [2.55956846]\n",
      "epoch: 429 loss: [2.5248482]\n",
      "epoch: 430 loss: [2.49058177]\n",
      "epoch: 431 loss: [2.45676336]\n",
      "epoch: 432 loss: [2.42338723]\n",
      "epoch: 433 loss: [2.39044771]\n",
      "epoch: 434 loss: [2.35793919]\n",
      "epoch: 435 loss: [2.32585617]\n",
      "epoch: 436 loss: [2.29419318]\n",
      "epoch: 437 loss: [2.26294483]\n",
      "epoch: 438 loss: [2.23210582]\n",
      "epoch: 439 loss: [2.20167088]\n",
      "epoch: 440 loss: [2.17163484]\n",
      "epoch: 441 loss: [2.14199257]\n",
      "epoch: 442 loss: [2.11273903]\n",
      "epoch: 443 loss: [2.08386922]\n",
      "epoch: 444 loss: [2.05537823]\n",
      "epoch: 445 loss: [2.02726118]\n",
      "epoch: 446 loss: [1.99951328]\n",
      "epoch: 447 loss: [1.97212979]\n",
      "epoch: 448 loss: [1.94510603]\n",
      "epoch: 449 loss: [1.91843738]\n",
      "epoch: 450 loss: [1.89211927]\n",
      "epoch: 451 loss: [1.8661472]\n",
      "epoch: 452 loss: [1.84051674]\n",
      "epoch: 453 loss: [1.81522348]\n",
      "epoch: 454 loss: [1.79026309]\n",
      "epoch: 455 loss: [1.7656313]\n",
      "epoch: 456 loss: [1.74132387]\n",
      "epoch: 457 loss: [1.71733665]\n",
      "epoch: 458 loss: [1.6936655]\n",
      "epoch: 459 loss: [1.67030638]\n",
      "epoch: 460 loss: [1.64725525]\n",
      "epoch: 461 loss: [1.62450816]\n",
      "epoch: 462 loss: [1.6020612]\n",
      "epoch: 463 loss: [1.5799105]\n",
      "epoch: 464 loss: [1.55805226]\n",
      "epoch: 465 loss: [1.5364827]\n",
      "epoch: 466 loss: [1.5151981]\n",
      "epoch: 467 loss: [1.49419481]\n",
      "epoch: 468 loss: [1.47346919]\n",
      "epoch: 469 loss: [1.45301767]\n",
      "epoch: 470 loss: [1.43283672]\n",
      "epoch: 471 loss: [1.41292285]\n",
      "epoch: 472 loss: [1.39327261]\n",
      "epoch: 473 loss: [1.37388262]\n",
      "epoch: 474 loss: [1.35474951]\n",
      "epoch: 475 loss: [1.33586997]\n",
      "epoch: 476 loss: [1.31724073]\n",
      "epoch: 477 loss: [1.29885857]\n",
      "epoch: 478 loss: [1.28072029]\n",
      "epoch: 479 loss: [1.26282276]\n",
      "epoch: 480 loss: [1.24516285]\n",
      "epoch: 481 loss: [1.22773751]\n",
      "epoch: 482 loss: [1.2105437]\n",
      "epoch: 483 loss: [1.19357844]\n",
      "epoch: 484 loss: [1.17683878]\n",
      "epoch: 485 loss: [1.1603218]\n",
      "epoch: 486 loss: [1.14402462]\n",
      "epoch: 487 loss: [1.12794441]\n",
      "epoch: 488 loss: [1.11207835]\n",
      "epoch: 489 loss: [1.09642369]\n",
      "epoch: 490 loss: [1.08097768]\n",
      "epoch: 491 loss: [1.06573764]\n",
      "epoch: 492 loss: [1.05070089]\n",
      "epoch: 493 loss: [1.03586481]\n",
      "epoch: 494 loss: [1.0212268]\n",
      "epoch: 495 loss: [1.0067843]\n",
      "epoch: 496 loss: [0.99253477]\n",
      "epoch: 497 loss: [0.97847573]\n",
      "epoch: 498 loss: [0.96460469]\n",
      "epoch: 499 loss: [0.95091924]\n",
      "epoch: 500 loss: [0.93741695]\n",
      "epoch: 501 loss: [0.92409547]\n",
      "epoch: 502 loss: [0.91095244]\n",
      "epoch: 503 loss: [0.89798556]\n",
      "epoch: 504 loss: [0.88519254]\n",
      "epoch: 505 loss: [0.87257113]\n",
      "epoch: 506 loss: [0.86011909]\n",
      "epoch: 507 loss: [0.84783424]\n",
      "epoch: 508 loss: [0.83571441]\n",
      "epoch: 509 loss: [0.82375745]\n",
      "epoch: 510 loss: [0.81196125]\n",
      "epoch: 511 loss: [0.80032372]\n",
      "epoch: 512 loss: [0.7888428]\n",
      "epoch: 513 loss: [0.77751646]\n",
      "epoch: 514 loss: [0.76634269]\n",
      "epoch: 515 loss: [0.7553195]\n",
      "epoch: 516 loss: [0.74444495]\n",
      "epoch: 517 loss: [0.73371709]\n",
      "epoch: 518 loss: [0.72313403]\n",
      "epoch: 519 loss: [0.71269387]\n",
      "epoch: 520 loss: [0.70239476]\n",
      "epoch: 521 loss: [0.69223486]\n",
      "epoch: 522 loss: [0.68221237]\n",
      "epoch: 523 loss: [0.67232549]\n",
      "epoch: 524 loss: [0.66257245]\n",
      "epoch: 525 loss: [0.65295152]\n",
      "epoch: 526 loss: [0.64346097]\n",
      "epoch: 527 loss: [0.63409911]\n",
      "epoch: 528 loss: [0.62486425]\n",
      "epoch: 529 loss: [0.61575475]\n",
      "epoch: 530 loss: [0.60676896]\n",
      "epoch: 531 loss: [0.59790527]\n",
      "epoch: 532 loss: [0.5891621]\n",
      "epoch: 533 loss: [0.58053787]\n",
      "epoch: 534 loss: [0.57203102]\n",
      "epoch: 535 loss: [0.56364002]\n",
      "epoch: 536 loss: [0.55536337]\n",
      "epoch: 537 loss: [0.54719956]\n",
      "epoch: 538 loss: [0.53914712]\n",
      "epoch: 539 loss: [0.53120461]\n",
      "epoch: 540 loss: [0.52337057]\n",
      "epoch: 541 loss: [0.51564359]\n",
      "epoch: 542 loss: [0.50802228]\n",
      "epoch: 543 loss: [0.50050525]\n",
      "epoch: 544 loss: [0.49309113]\n",
      "epoch: 545 loss: [0.48577858]\n",
      "epoch: 546 loss: [0.47856627]\n",
      "epoch: 547 loss: [0.47145288]\n",
      "epoch: 548 loss: [0.46443713]\n",
      "epoch: 549 loss: [0.45751772]\n",
      "epoch: 550 loss: [0.4506934]\n",
      "epoch: 551 loss: [0.44396292]\n",
      "epoch: 552 loss: [0.43732506]\n",
      "epoch: 553 loss: [0.43077858]\n",
      "epoch: 554 loss: [0.42432231]\n",
      "epoch: 555 loss: [0.41795505]\n",
      "epoch: 556 loss: [0.41167562]\n",
      "epoch: 557 loss: [0.40548289]\n",
      "epoch: 558 loss: [0.39937572]\n",
      "epoch: 559 loss: [0.39335296]\n",
      "epoch: 560 loss: [0.38741353]\n",
      "epoch: 561 loss: [0.38155631]\n",
      "epoch: 562 loss: [0.37578024]\n",
      "epoch: 563 loss: [0.37008424]\n",
      "epoch: 564 loss: [0.36446725]\n",
      "epoch: 565 loss: [0.35892825]\n",
      "epoch: 566 loss: [0.35346619]\n",
      "epoch: 567 loss: [0.34808008]\n",
      "epoch: 568 loss: [0.3427689]\n",
      "epoch: 569 loss: [0.33753167]\n",
      "epoch: 570 loss: [0.33236741]\n",
      "epoch: 571 loss: [0.32727517]\n",
      "epoch: 572 loss: [0.32225399]\n",
      "epoch: 573 loss: [0.31730293]\n",
      "epoch: 574 loss: [0.31242106]\n",
      "epoch: 575 loss: [0.30760749]\n",
      "epoch: 576 loss: [0.30286129]\n",
      "epoch: 577 loss: [0.29818158]\n",
      "epoch: 578 loss: [0.29356749]\n",
      "epoch: 579 loss: [0.28901814]\n",
      "epoch: 580 loss: [0.28453268]\n",
      "epoch: 581 loss: [0.28011026]\n",
      "epoch: 582 loss: [0.27575005]\n",
      "epoch: 583 loss: [0.27145122]\n",
      "epoch: 584 loss: [0.26721296]\n",
      "epoch: 585 loss: [0.26303447]\n",
      "epoch: 586 loss: [0.25891495]\n",
      "epoch: 587 loss: [0.25485363]\n",
      "epoch: 588 loss: [0.25084973]\n",
      "epoch: 589 loss: [0.24690248]\n",
      "epoch: 590 loss: [0.24301115]\n",
      "epoch: 591 loss: [0.23917497]\n",
      "epoch: 592 loss: [0.23539323]\n",
      "epoch: 593 loss: [0.23166519]\n",
      "epoch: 594 loss: [0.22799015]\n",
      "epoch: 595 loss: [0.2243674]\n",
      "epoch: 596 loss: [0.22079623]\n",
      "epoch: 597 loss: [0.21727598]\n",
      "epoch: 598 loss: [0.21380595]\n",
      "epoch: 599 loss: [0.21038548]\n",
      "epoch: 600 loss: [0.2070139]\n",
      "epoch: 601 loss: [0.20369058]\n",
      "epoch: 602 loss: [0.20041485]\n",
      "epoch: 603 loss: [0.19718609]\n",
      "epoch: 604 loss: [0.19400367]\n",
      "epoch: 605 loss: [0.19086697]\n",
      "epoch: 606 loss: [0.18777538]\n",
      "epoch: 607 loss: [0.1847283]\n",
      "epoch: 608 loss: [0.18172513]\n",
      "epoch: 609 loss: [0.17876528]\n",
      "epoch: 610 loss: [0.17584818]\n",
      "epoch: 611 loss: [0.17297325]\n",
      "epoch: 612 loss: [0.17013992]\n",
      "epoch: 613 loss: [0.16734765]\n",
      "epoch: 614 loss: [0.16459587]\n",
      "epoch: 615 loss: [0.16188405]\n",
      "epoch: 616 loss: [0.15921165]\n",
      "epoch: 617 loss: [0.15657814]\n",
      "epoch: 618 loss: [0.15398299]\n",
      "epoch: 619 loss: [0.1514257]\n",
      "epoch: 620 loss: [0.14890575]\n",
      "epoch: 621 loss: [0.14642264]\n",
      "epoch: 622 loss: [0.14397588]\n",
      "epoch: 623 loss: [0.14156497]\n",
      "epoch: 624 loss: [0.13918944]\n",
      "epoch: 625 loss: [0.1368488]\n",
      "epoch: 626 loss: [0.13454259]\n",
      "epoch: 627 loss: [0.13227033]\n",
      "epoch: 628 loss: [0.13003158]\n",
      "epoch: 629 loss: [0.12782588]\n",
      "epoch: 630 loss: [0.12565279]\n",
      "epoch: 631 loss: [0.12351186]\n",
      "epoch: 632 loss: [0.12140265]\n",
      "epoch: 633 loss: [0.11932474]\n",
      "epoch: 634 loss: [0.11727771]\n",
      "epoch: 635 loss: [0.11526114]\n",
      "epoch: 636 loss: [0.11327461]\n",
      "epoch: 637 loss: [0.11131771]\n",
      "epoch: 638 loss: [0.10939005]\n",
      "epoch: 639 loss: [0.10749123]\n",
      "epoch: 640 loss: [0.10562086]\n",
      "epoch: 641 loss: [0.10377854]\n",
      "epoch: 642 loss: [0.1019639]\n",
      "epoch: 643 loss: [0.10017656]\n",
      "epoch: 644 loss: [0.09841616]\n",
      "epoch: 645 loss: [0.09668231]\n",
      "epoch: 646 loss: [0.09497467]\n",
      "epoch: 647 loss: [0.09329287]\n",
      "epoch: 648 loss: [0.09163656]\n",
      "epoch: 649 loss: [0.0900054]\n",
      "epoch: 650 loss: [0.08839903]\n",
      "epoch: 651 loss: [0.08681713]\n",
      "epoch: 652 loss: [0.08525935]\n",
      "epoch: 653 loss: [0.08372537]\n",
      "epoch: 654 loss: [0.08221486]\n",
      "epoch: 655 loss: [0.0807275]\n",
      "epoch: 656 loss: [0.07926297]\n",
      "epoch: 657 loss: [0.07782096]\n",
      "epoch: 658 loss: [0.07640116]\n",
      "epoch: 659 loss: [0.07500326]\n",
      "epoch: 660 loss: [0.07362697]\n",
      "epoch: 661 loss: [0.07227198]\n",
      "epoch: 662 loss: [0.07093801]\n",
      "epoch: 663 loss: [0.06962476]\n",
      "epoch: 664 loss: [0.06833195]\n",
      "epoch: 665 loss: [0.06705929]\n",
      "epoch: 666 loss: [0.06580651]\n",
      "epoch: 667 loss: [0.06457334]\n",
      "epoch: 668 loss: [0.0633595]\n",
      "epoch: 669 loss: [0.06216473]\n",
      "epoch: 670 loss: [0.06098876]\n",
      "epoch: 671 loss: [0.05983133]\n",
      "epoch: 672 loss: [0.05869218]\n",
      "epoch: 673 loss: [0.05757107]\n",
      "epoch: 674 loss: [0.05646774]\n",
      "epoch: 675 loss: [0.05538194]\n",
      "epoch: 676 loss: [0.05431343]\n",
      "epoch: 677 loss: [0.05326198]\n",
      "epoch: 678 loss: [0.05222733]\n",
      "epoch: 679 loss: [0.05120927]\n",
      "epoch: 680 loss: [0.05020756]\n",
      "epoch: 681 loss: [0.04922196]\n",
      "epoch: 682 loss: [0.04825226]\n",
      "epoch: 683 loss: [0.04729824]\n",
      "epoch: 684 loss: [0.04635966]\n",
      "epoch: 685 loss: [0.04543633]\n",
      "epoch: 686 loss: [0.04452802]\n",
      "epoch: 687 loss: [0.04363453]\n",
      "epoch: 688 loss: [0.04275564]\n",
      "epoch: 689 loss: [0.04189115]\n",
      "epoch: 690 loss: [0.04104086]\n",
      "epoch: 691 loss: [0.04020458]\n",
      "epoch: 692 loss: [0.03938209]\n",
      "epoch: 693 loss: [0.03857321]\n",
      "epoch: 694 loss: [0.03777775]\n",
      "epoch: 695 loss: [0.03699552]\n",
      "epoch: 696 loss: [0.03622633]\n",
      "epoch: 697 loss: [0.03547]\n",
      "epoch: 698 loss: [0.03472634]\n",
      "epoch: 699 loss: [0.03399517]\n",
      "epoch: 700 loss: [0.03327633]\n",
      "epoch: 701 loss: [0.03256963]\n",
      "epoch: 702 loss: [0.0318749]\n",
      "epoch: 703 loss: [0.03119197]\n",
      "epoch: 704 loss: [0.03052067]\n",
      "epoch: 705 loss: [0.02986085]\n",
      "epoch: 706 loss: [0.02921233]\n",
      "epoch: 707 loss: [0.02857495]\n",
      "epoch: 708 loss: [0.02794856]\n",
      "epoch: 709 loss: [0.02733299]\n",
      "epoch: 710 loss: [0.0267281]\n",
      "epoch: 711 loss: [0.02613373]\n",
      "epoch: 712 loss: [0.02554973]\n",
      "epoch: 713 loss: [0.02497595]\n",
      "epoch: 714 loss: [0.02441224]\n",
      "epoch: 715 loss: [0.02385847]\n",
      "epoch: 716 loss: [0.02331448]\n",
      "epoch: 717 loss: [0.02278014]\n",
      "epoch: 718 loss: [0.0222553]\n",
      "epoch: 719 loss: [0.02173983]\n",
      "epoch: 720 loss: [0.0212336]\n",
      "epoch: 721 loss: [0.02073647]\n",
      "epoch: 722 loss: [0.02024831]\n",
      "epoch: 723 loss: [0.01976899]\n",
      "epoch: 724 loss: [0.01929837]\n",
      "epoch: 725 loss: [0.01883635]\n",
      "epoch: 726 loss: [0.01838278]\n",
      "epoch: 727 loss: [0.01793755]\n",
      "epoch: 728 loss: [0.01750054]\n",
      "epoch: 729 loss: [0.01707162]\n",
      "epoch: 730 loss: [0.01665067]\n",
      "epoch: 731 loss: [0.01623759]\n",
      "epoch: 732 loss: [0.01583226]\n",
      "epoch: 733 loss: [0.01543455]\n",
      "epoch: 734 loss: [0.01504437]\n",
      "epoch: 735 loss: [0.01466159]\n",
      "epoch: 736 loss: [0.01428612]\n",
      "epoch: 737 loss: [0.01391784]\n",
      "epoch: 738 loss: [0.01355665]\n",
      "epoch: 739 loss: [0.01320244]\n",
      "epoch: 740 loss: [0.0128551]\n",
      "epoch: 741 loss: [0.01251455]\n",
      "epoch: 742 loss: [0.01218067]\n",
      "epoch: 743 loss: [0.01185337]\n",
      "epoch: 744 loss: [0.01153255]\n",
      "epoch: 745 loss: [0.01121812]\n",
      "epoch: 746 loss: [0.01090997]\n",
      "epoch: 747 loss: [0.01060802]\n",
      "epoch: 748 loss: [0.01031217]\n",
      "epoch: 749 loss: [0.01002233]\n",
      "epoch: 750 loss: [0.00973841]\n",
      "epoch: 751 loss: [0.00946033]\n",
      "epoch: 752 loss: [0.00918798]\n",
      "epoch: 753 loss: [0.0089213]\n",
      "epoch: 754 loss: [0.00866019]\n",
      "epoch: 755 loss: [0.00840457]\n",
      "epoch: 756 loss: [0.00815435]\n",
      "epoch: 757 loss: [0.00790946]\n",
      "epoch: 758 loss: [0.00766981]\n",
      "epoch: 759 loss: [0.00743533]\n",
      "epoch: 760 loss: [0.00720593]\n",
      "epoch: 761 loss: [0.00698154]\n",
      "epoch: 762 loss: [0.00676207]\n",
      "epoch: 763 loss: [0.00654747]\n",
      "epoch: 764 loss: [0.00633765]\n",
      "epoch: 765 loss: [0.00613253]\n",
      "epoch: 766 loss: [0.00593205]\n",
      "epoch: 767 loss: [0.00573614]\n",
      "epoch: 768 loss: [0.00554472]\n",
      "epoch: 769 loss: [0.00535773]\n",
      "epoch: 770 loss: [0.00517509]\n",
      "epoch: 771 loss: [0.00499674]\n",
      "epoch: 772 loss: [0.00482262]\n",
      "epoch: 773 loss: [0.00465265]\n",
      "epoch: 774 loss: [0.00448678]\n",
      "epoch: 775 loss: [0.00432493]\n",
      "epoch: 776 loss: [0.00416705]\n",
      "epoch: 777 loss: [0.00401308]\n",
      "epoch: 778 loss: [0.00386294]\n",
      "epoch: 779 loss: [0.00371659]\n",
      "epoch: 780 loss: [0.00357396]\n",
      "epoch: 781 loss: [0.003435]\n",
      "epoch: 782 loss: [0.00329964]\n",
      "epoch: 783 loss: [0.00316783]\n",
      "epoch: 784 loss: [0.00303952]\n",
      "epoch: 785 loss: [0.00291464]\n",
      "epoch: 786 loss: [0.00279314]\n",
      "epoch: 787 loss: [0.00267497]\n",
      "epoch: 788 loss: [0.00256008]\n",
      "epoch: 789 loss: [0.00244841]\n",
      "epoch: 790 loss: [0.00233991]\n",
      "epoch: 791 loss: [0.00223453]\n",
      "epoch: 792 loss: [0.00213222]\n",
      "epoch: 793 loss: [0.00203293]\n",
      "epoch: 794 loss: [0.00193662]\n",
      "epoch: 795 loss: [0.00184322]\n",
      "epoch: 796 loss: [0.0017527]\n",
      "epoch: 797 loss: [0.00166501]\n",
      "epoch: 798 loss: [0.0015801]\n",
      "epoch: 799 loss: [0.00149793]\n",
      "epoch: 800 loss: [0.00141845]\n",
      "epoch: 801 loss: [0.00134162]\n",
      "epoch: 802 loss: [0.00126739]\n",
      "epoch: 803 loss: [0.00119572]\n",
      "epoch: 804 loss: [0.00112657]\n",
      "epoch: 805 loss: [0.0010599]\n",
      "epoch: 806 loss: [0.00099566]\n",
      "epoch: 807 loss: [0.00093382]\n",
      "epoch: 808 loss: [0.00087433]\n",
      "epoch: 809 loss: [0.00081715]\n",
      "epoch: 810 loss: [0.00076225]\n",
      "epoch: 811 loss: [0.00070959]\n",
      "epoch: 812 loss: [0.00065913]\n",
      "epoch: 813 loss: [0.00061082]\n",
      "epoch: 814 loss: [0.00056465]\n",
      "epoch: 815 loss: [0.00052056]\n",
      "epoch: 816 loss: [0.00047853]\n",
      "epoch: 817 loss: [0.00043851]\n",
      "epoch: 818 loss: [0.00040048]\n",
      "epoch: 819 loss: [0.00036439]\n",
      "epoch: 820 loss: [0.00033022]\n",
      "epoch: 821 loss: [0.00029793]\n",
      "epoch: 822 loss: [0.00026749]\n",
      "epoch: 823 loss: [0.00023887]\n",
      "epoch: 824 loss: [0.00021203]\n",
      "epoch: 825 loss: [0.00018695]\n",
      "epoch: 826 loss: [0.00016358]\n",
      "epoch: 827 loss: [0.00014191]\n",
      "epoch: 828 loss: [0.0001219]\n",
      "epoch: 829 loss: [0.00010352]\n",
      "epoch: 830 loss: [8.67477914e-05]\n",
      "epoch: 831 loss: [7.15458691e-05]\n",
      "epoch: 832 loss: [5.78893524e-05]\n",
      "epoch: 833 loss: [4.57502596e-05]\n",
      "epoch: 834 loss: [3.51010149e-05]\n",
      "epoch: 835 loss: [2.5914443e-05]\n",
      "epoch: 836 loss: [1.81637635e-05]\n",
      "epoch: 837 loss: [1.18225859e-05]\n",
      "epoch: 838 loss: [6.86490385e-06]\n",
      "epoch: 839 loss: [3.26509063e-06]\n",
      "epoch: 840 loss: [9.97893604e-07]\n",
      "epoch: 841 loss: [3.84293267e-08]\n",
      "epoch: 842 loss: [3.62178574e-07]\n",
      "epoch: 843 loss: [1.94498143e-06]\n",
      "epoch: 844 loss: [4.76303245e-06]\n",
      "epoch: 845 loss: [8.79287588e-06]\n",
      "epoch: 846 loss: [1.40114009e-05]\n",
      "epoch: 847 loss: [2.03958372e-05]\n",
      "epoch: 848 loss: [2.79237498e-05]\n",
      "epoch: 849 loss: [3.65730354e-05]\n",
      "epoch: 850 loss: [4.63219171e-05]\n",
      "epoch: 851 loss: [5.71489404e-05]\n",
      "epoch: 852 loss: [6.90329687e-05]\n",
      "epoch: 853 loss: [8.19531791e-05]\n",
      "epoch: 854 loss: [9.58890581e-05]\n",
      "epoch: 855 loss: [0.00011082]\n",
      "epoch: 856 loss: [0.00012673]\n",
      "epoch: 857 loss: [0.00014359]\n",
      "epoch: 858 loss: [0.00016139]\n",
      "epoch: 859 loss: [0.00018011]\n",
      "epoch: 860 loss: [0.00019972]\n",
      "epoch: 861 loss: [0.00022022]\n",
      "epoch: 862 loss: [0.00024158]\n",
      "epoch: 863 loss: [0.00026378]\n",
      "epoch: 864 loss: [0.00028681]\n",
      "epoch: 865 loss: [0.00031065]\n",
      "epoch: 866 loss: [0.00033528]\n",
      "epoch: 867 loss: [0.00036069]\n",
      "epoch: 868 loss: [0.00038685]\n",
      "epoch: 869 loss: [0.00041376]\n",
      "epoch: 870 loss: [0.00044139]\n",
      "epoch: 871 loss: [0.00046974]\n",
      "epoch: 872 loss: [0.00049877]\n",
      "epoch: 873 loss: [0.00052849]\n",
      "epoch: 874 loss: [0.00055887]\n",
      "epoch: 875 loss: [0.00058991]\n",
      "epoch: 876 loss: [0.00062157]\n",
      "epoch: 877 loss: [0.00065386]\n",
      "epoch: 878 loss: [0.00068675]\n",
      "epoch: 879 loss: [0.00072024]\n",
      "epoch: 880 loss: [0.0007543]\n",
      "epoch: 881 loss: [0.00078893]\n",
      "epoch: 882 loss: [0.00082411]\n",
      "epoch: 883 loss: [0.00085983]\n",
      "epoch: 884 loss: [0.00089607]\n",
      "epoch: 885 loss: [0.00093282]\n",
      "epoch: 886 loss: [0.00097008]\n",
      "epoch: 887 loss: [0.00100782]\n",
      "epoch: 888 loss: [0.00104604]\n",
      "epoch: 889 loss: [0.00108472]\n",
      "epoch: 890 loss: [0.00112385]\n",
      "epoch: 891 loss: [0.00116342]\n",
      "epoch: 892 loss: [0.00120343]\n",
      "epoch: 893 loss: [0.00124384]\n",
      "epoch: 894 loss: [0.00128467]\n",
      "epoch: 895 loss: [0.00132589]\n",
      "epoch: 896 loss: [0.00136749]\n",
      "epoch: 897 loss: [0.00140947]\n",
      "epoch: 898 loss: [0.00145182]\n",
      "epoch: 899 loss: [0.00149451]\n",
      "epoch: 900 loss: [0.00153755]\n",
      "epoch: 901 loss: [0.00158093]\n",
      "epoch: 902 loss: [0.00162462]\n",
      "epoch: 903 loss: [0.00166863]\n",
      "epoch: 904 loss: [0.00171295]\n",
      "epoch: 905 loss: [0.00175756]\n",
      "epoch: 906 loss: [0.00180246]\n",
      "epoch: 907 loss: [0.00184764]\n",
      "epoch: 908 loss: [0.00189309]\n",
      "epoch: 909 loss: [0.00193879]\n",
      "epoch: 910 loss: [0.00198475]\n",
      "epoch: 911 loss: [0.00203095]\n",
      "epoch: 912 loss: [0.00207739]\n",
      "epoch: 913 loss: [0.00212405]\n",
      "epoch: 914 loss: [0.00217093]\n",
      "epoch: 915 loss: [0.00221803]\n",
      "epoch: 916 loss: [0.00226532]\n",
      "epoch: 917 loss: [0.00231281]\n",
      "epoch: 918 loss: [0.0023605]\n",
      "epoch: 919 loss: [0.00240836]\n",
      "epoch: 920 loss: [0.00245639]\n",
      "epoch: 921 loss: [0.0025046]\n",
      "epoch: 922 loss: [0.00255296]\n",
      "epoch: 923 loss: [0.00260147]\n",
      "epoch: 924 loss: [0.00265013]\n",
      "epoch: 925 loss: [0.00269894]\n",
      "epoch: 926 loss: [0.00274787]\n",
      "epoch: 927 loss: [0.00279693]\n",
      "epoch: 928 loss: [0.00284611]\n",
      "epoch: 929 loss: [0.00289541]\n",
      "epoch: 930 loss: [0.00294481]\n",
      "epoch: 931 loss: [0.00299432]\n",
      "epoch: 932 loss: [0.00304392]\n",
      "epoch: 933 loss: [0.00309362]\n",
      "epoch: 934 loss: [0.0031434]\n",
      "epoch: 935 loss: [0.00319325]\n",
      "epoch: 936 loss: [0.00324319]\n",
      "epoch: 937 loss: [0.00329319]\n",
      "epoch: 938 loss: [0.00334325]\n",
      "epoch: 939 loss: [0.00339337]\n",
      "epoch: 940 loss: [0.00344355]\n",
      "epoch: 941 loss: [0.00349377]\n",
      "epoch: 942 loss: [0.00354404]\n",
      "epoch: 943 loss: [0.00359435]\n",
      "epoch: 944 loss: [0.00364469]\n",
      "epoch: 945 loss: [0.00369505]\n",
      "epoch: 946 loss: [0.00374545]\n",
      "epoch: 947 loss: [0.00379586]\n",
      "epoch: 948 loss: [0.00384629]\n",
      "epoch: 949 loss: [0.00389673]\n",
      "epoch: 950 loss: [0.00394718]\n",
      "epoch: 951 loss: [0.00399763]\n",
      "epoch: 952 loss: [0.00404808]\n",
      "epoch: 953 loss: [0.00409852]\n",
      "epoch: 954 loss: [0.00414896]\n",
      "epoch: 955 loss: [0.00419938]\n",
      "epoch: 956 loss: [0.00424979]\n",
      "epoch: 957 loss: [0.00430018]\n",
      "epoch: 958 loss: [0.00435054]\n",
      "epoch: 959 loss: [0.00440088]\n",
      "epoch: 960 loss: [0.00445118]\n",
      "epoch: 961 loss: [0.00450145]\n",
      "epoch: 962 loss: [0.00455169]\n",
      "epoch: 963 loss: [0.00460188]\n",
      "epoch: 964 loss: [0.00465203]\n",
      "epoch: 965 loss: [0.00470213]\n",
      "epoch: 966 loss: [0.00475218]\n",
      "epoch: 967 loss: [0.00480218]\n",
      "epoch: 968 loss: [0.00485212]\n",
      "epoch: 969 loss: [0.004902]\n",
      "epoch: 970 loss: [0.00495182]\n",
      "epoch: 971 loss: [0.00500158]\n",
      "epoch: 972 loss: [0.00505127]\n",
      "epoch: 973 loss: [0.00510088]\n",
      "epoch: 974 loss: [0.00515043]\n",
      "epoch: 975 loss: [0.0051999]\n",
      "epoch: 976 loss: [0.00524929]\n",
      "epoch: 977 loss: [0.0052986]\n",
      "epoch: 978 loss: [0.00534783]\n",
      "epoch: 979 loss: [0.00539697]\n",
      "epoch: 980 loss: [0.00544603]\n",
      "epoch: 981 loss: [0.00549499]\n",
      "epoch: 982 loss: [0.00554387]\n",
      "epoch: 983 loss: [0.00559265]\n",
      "epoch: 984 loss: [0.00564133]\n",
      "epoch: 985 loss: [0.00568992]\n",
      "epoch: 986 loss: [0.00573841]\n",
      "epoch: 987 loss: [0.00578679]\n",
      "epoch: 988 loss: [0.00583507]\n",
      "epoch: 989 loss: [0.00588324]\n",
      "epoch: 990 loss: [0.00593131]\n",
      "epoch: 991 loss: [0.00597927]\n",
      "epoch: 992 loss: [0.00602711]\n",
      "epoch: 993 loss: [0.00607484]\n",
      "epoch: 994 loss: [0.00612246]\n",
      "epoch: 995 loss: [0.00616996]\n",
      "epoch: 996 loss: [0.00621735]\n",
      "epoch: 997 loss: [0.00626461]\n",
      "epoch: 998 loss: [0.00631175]\n",
      "epoch: 999 loss: [0.00635877]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> -0.38812046755534374 err: [0.38812047]\n",
      "[0. 1.] [0.] -> 0.22664478763054247 err: [-0.22664479]\n",
      "[1. 0.] [0.] -> 0.23053542483961215 err: [-0.23053542]\n",
      "[1. 1.] [1.] -> 0.8453006800254983 err: [0.15469932]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [0.] -> 0 err: [0.]\n",
      "[1. 0.] [0.] -> 0 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 練習:「OR」を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [12.32774644]\n",
      "epoch: 1 loss: [11.16905281]\n",
      "epoch: 2 loss: [10.1329487]\n",
      "epoch: 3 loss: [9.20573876]\n",
      "epoch: 4 loss: [8.37530849]\n",
      "epoch: 5 loss: [7.63093808]\n",
      "epoch: 6 loss: [6.96313848]\n",
      "epoch: 7 loss: [6.36350693]\n",
      "epoch: 8 loss: [5.82459971]\n",
      "epoch: 9 loss: [5.33982001]\n",
      "epoch: 10 loss: [4.90331904]\n",
      "epoch: 11 loss: [4.50990896]\n",
      "epoch: 12 loss: [4.154986]\n",
      "epoch: 13 loss: [3.83446281]\n",
      "epoch: 14 loss: [3.54470862]\n",
      "epoch: 15 loss: [3.28249663]\n",
      "epoch: 16 loss: [3.04495748]\n",
      "epoch: 17 loss: [2.82953824]\n",
      "epoch: 18 loss: [2.63396617]\n",
      "epoch: 19 loss: [2.45621678]\n",
      "epoch: 20 loss: [2.29448556]\n",
      "epoch: 21 loss: [2.14716309]\n",
      "epoch: 22 loss: [2.01281297]\n",
      "epoch: 23 loss: [1.89015233]\n",
      "epoch: 24 loss: [1.77803465]\n",
      "epoch: 25 loss: [1.67543453]\n",
      "epoch: 26 loss: [1.5814342]\n",
      "epoch: 27 loss: [1.49521159]\n",
      "epoch: 28 loss: [1.41602983]\n",
      "epoch: 29 loss: [1.34322786]\n",
      "epoch: 30 loss: [1.27621214]\n",
      "epoch: 31 loss: [1.21444937]\n",
      "epoch: 32 loss: [1.15745994]\n",
      "epoch: 33 loss: [1.10481216]\n",
      "epoch: 34 loss: [1.05611715]\n",
      "epoch: 35 loss: [1.01102428]\n",
      "epoch: 36 loss: [0.96921717]\n",
      "epoch: 37 loss: [0.93041002]\n",
      "epoch: 38 loss: [0.8943445]\n",
      "epoch: 39 loss: [0.86078686]\n",
      "epoch: 40 loss: [0.82952539]\n",
      "epoch: 41 loss: [0.80036823]\n",
      "epoch: 42 loss: [0.77314127]\n",
      "epoch: 43 loss: [0.74768643]\n",
      "epoch: 44 loss: [0.72386005]\n",
      "epoch: 45 loss: [0.70153142]\n",
      "epoch: 46 loss: [0.68058154]\n",
      "epoch: 47 loss: [0.66090197]\n",
      "epoch: 48 loss: [0.6423938]\n",
      "epoch: 49 loss: [0.62496674]\n",
      "epoch: 50 loss: [0.60853829]\n",
      "epoch: 51 loss: [0.59303304]\n",
      "epoch: 52 loss: [0.57838194]\n",
      "epoch: 53 loss: [0.56452177]\n",
      "epoch: 54 loss: [0.55139458]\n",
      "epoch: 55 loss: [0.5389472]\n",
      "epoch: 56 loss: [0.52713082]\n",
      "epoch: 57 loss: [0.5159006]\n",
      "epoch: 58 loss: [0.50521529]\n",
      "epoch: 59 loss: [0.49503698]\n",
      "epoch: 60 loss: [0.48533071]\n",
      "epoch: 61 loss: [0.47606432]\n",
      "epoch: 62 loss: [0.46720811]\n",
      "epoch: 63 loss: [0.4587347]\n",
      "epoch: 64 loss: [0.45061883]\n",
      "epoch: 65 loss: [0.44283713]\n",
      "epoch: 66 loss: [0.43536802]\n",
      "epoch: 67 loss: [0.42819152]\n",
      "epoch: 68 loss: [0.42128916]\n",
      "epoch: 69 loss: [0.41464383]\n",
      "epoch: 70 loss: [0.40823966]\n",
      "epoch: 71 loss: [0.40206196]\n",
      "epoch: 72 loss: [0.39609709]\n",
      "epoch: 73 loss: [0.3903324]\n",
      "epoch: 74 loss: [0.38475613]\n",
      "epoch: 75 loss: [0.37935737]\n",
      "epoch: 76 loss: [0.37412598]\n",
      "epoch: 77 loss: [0.36905251]\n",
      "epoch: 78 loss: [0.3641282]\n",
      "epoch: 79 loss: [0.35934486]\n",
      "epoch: 80 loss: [0.35469488]\n",
      "epoch: 81 loss: [0.35017118]\n",
      "epoch: 82 loss: [0.34576714]\n",
      "epoch: 83 loss: [0.34147661]\n",
      "epoch: 84 loss: [0.33729381]\n",
      "epoch: 85 loss: [0.3332134]\n",
      "epoch: 86 loss: [0.32923034]\n",
      "epoch: 87 loss: [0.32533996]\n",
      "epoch: 88 loss: [0.32153788]\n",
      "epoch: 89 loss: [0.31781999]\n",
      "epoch: 90 loss: [0.31418246]\n",
      "epoch: 91 loss: [0.3106217]\n",
      "epoch: 92 loss: [0.30713435]\n",
      "epoch: 93 loss: [0.30371726]\n",
      "epoch: 94 loss: [0.30036748]\n",
      "epoch: 95 loss: [0.29708223]\n",
      "epoch: 96 loss: [0.29385891]\n",
      "epoch: 97 loss: [0.29069508]\n",
      "epoch: 98 loss: [0.28758844]\n",
      "epoch: 99 loss: [0.28453684]\n",
      "epoch: 100 loss: [0.28153824]\n",
      "epoch: 101 loss: [0.27859073]\n",
      "epoch: 102 loss: [0.27569252]\n",
      "epoch: 103 loss: [0.27284191]\n",
      "epoch: 104 loss: [0.2700373]\n",
      "epoch: 105 loss: [0.26727717]\n",
      "epoch: 106 loss: [0.26456011]\n",
      "epoch: 107 loss: [0.26188477]\n",
      "epoch: 108 loss: [0.25924987]\n",
      "epoch: 109 loss: [0.25665421]\n",
      "epoch: 110 loss: [0.25409666]\n",
      "epoch: 111 loss: [0.25157612]\n",
      "epoch: 112 loss: [0.24909158]\n",
      "epoch: 113 loss: [0.24664206]\n",
      "epoch: 114 loss: [0.24422664]\n",
      "epoch: 115 loss: [0.24184445]\n",
      "epoch: 116 loss: [0.23949464]\n",
      "epoch: 117 loss: [0.23717643]\n",
      "epoch: 118 loss: [0.23488905]\n",
      "epoch: 119 loss: [0.23263179]\n",
      "epoch: 120 loss: [0.23040396]\n",
      "epoch: 121 loss: [0.2282049]\n",
      "epoch: 122 loss: [0.22603398]\n",
      "epoch: 123 loss: [0.2238906]\n",
      "epoch: 124 loss: [0.22177419]\n",
      "epoch: 125 loss: [0.21968419]\n",
      "epoch: 126 loss: [0.21762008]\n",
      "epoch: 127 loss: [0.21558134]\n",
      "epoch: 128 loss: [0.2135675]\n",
      "epoch: 129 loss: [0.21157807]\n",
      "epoch: 130 loss: [0.20961262]\n",
      "epoch: 131 loss: [0.2076707]\n",
      "epoch: 132 loss: [0.2057519]\n",
      "epoch: 133 loss: [0.20385581]\n",
      "epoch: 134 loss: [0.20198205]\n",
      "epoch: 135 loss: [0.20013023]\n",
      "epoch: 136 loss: [0.1983]\n",
      "epoch: 137 loss: [0.19649099]\n",
      "epoch: 138 loss: [0.19470287]\n",
      "epoch: 139 loss: [0.19293531]\n",
      "epoch: 140 loss: [0.19118799]\n",
      "epoch: 141 loss: [0.18946059]\n",
      "epoch: 142 loss: [0.18775282]\n",
      "epoch: 143 loss: [0.18606437]\n",
      "epoch: 144 loss: [0.18439497]\n",
      "epoch: 145 loss: [0.18274432]\n",
      "epoch: 146 loss: [0.18111218]\n",
      "epoch: 147 loss: [0.17949826]\n",
      "epoch: 148 loss: [0.17790231]\n",
      "epoch: 149 loss: [0.17632408]\n",
      "epoch: 150 loss: [0.17476333]\n",
      "epoch: 151 loss: [0.17321981]\n",
      "epoch: 152 loss: [0.17169329]\n",
      "epoch: 153 loss: [0.17018355]\n",
      "epoch: 154 loss: [0.16869035]\n",
      "epoch: 155 loss: [0.16721348]\n",
      "epoch: 156 loss: [0.16575273]\n",
      "epoch: 157 loss: [0.16430789]\n",
      "epoch: 158 loss: [0.16287874]\n",
      "epoch: 159 loss: [0.16146509]\n",
      "epoch: 160 loss: [0.16006674]\n",
      "epoch: 161 loss: [0.15868349]\n",
      "epoch: 162 loss: [0.15731516]\n",
      "epoch: 163 loss: [0.15596155]\n",
      "epoch: 164 loss: [0.15462249]\n",
      "epoch: 165 loss: [0.1532978]\n",
      "epoch: 166 loss: [0.15198729]\n",
      "epoch: 167 loss: [0.15069079]\n",
      "epoch: 168 loss: [0.14940813]\n",
      "epoch: 169 loss: [0.14813914]\n",
      "epoch: 170 loss: [0.14688366]\n",
      "epoch: 171 loss: [0.14564153]\n",
      "epoch: 172 loss: [0.14441257]\n",
      "epoch: 173 loss: [0.14319664]\n",
      "epoch: 174 loss: [0.14199358]\n",
      "epoch: 175 loss: [0.14080324]\n",
      "epoch: 176 loss: [0.13962546]\n",
      "epoch: 177 loss: [0.1384601]\n",
      "epoch: 178 loss: [0.13730701]\n",
      "epoch: 179 loss: [0.13616604]\n",
      "epoch: 180 loss: [0.13503706]\n",
      "epoch: 181 loss: [0.13391992]\n",
      "epoch: 182 loss: [0.13281449]\n",
      "epoch: 183 loss: [0.13172063]\n",
      "epoch: 184 loss: [0.1306382]\n",
      "epoch: 185 loss: [0.12956707]\n",
      "epoch: 186 loss: [0.12850712]\n",
      "epoch: 187 loss: [0.12745821]\n",
      "epoch: 188 loss: [0.12642021]\n",
      "epoch: 189 loss: [0.12539301]\n",
      "epoch: 190 loss: [0.12437647]\n",
      "epoch: 191 loss: [0.12337047]\n",
      "epoch: 192 loss: [0.1223749]\n",
      "epoch: 193 loss: [0.12138964]\n",
      "epoch: 194 loss: [0.12041456]\n",
      "epoch: 195 loss: [0.11944956]\n",
      "epoch: 196 loss: [0.11849451]\n",
      "epoch: 197 loss: [0.1175493]\n",
      "epoch: 198 loss: [0.11661383]\n",
      "epoch: 199 loss: [0.11568798]\n",
      "epoch: 200 loss: [0.11477165]\n",
      "epoch: 201 loss: [0.11386472]\n",
      "epoch: 202 loss: [0.11296709]\n",
      "epoch: 203 loss: [0.11207866]\n",
      "epoch: 204 loss: [0.11119933]\n",
      "epoch: 205 loss: [0.11032898]\n",
      "epoch: 206 loss: [0.10946752]\n",
      "epoch: 207 loss: [0.10861486]\n",
      "epoch: 208 loss: [0.10777088]\n",
      "epoch: 209 loss: [0.1069355]\n",
      "epoch: 210 loss: [0.10610862]\n",
      "epoch: 211 loss: [0.10529015]\n",
      "epoch: 212 loss: [0.10447998]\n",
      "epoch: 213 loss: [0.10367803]\n",
      "epoch: 214 loss: [0.10288421]\n",
      "epoch: 215 loss: [0.10209843]\n",
      "epoch: 216 loss: [0.10132059]\n",
      "epoch: 217 loss: [0.10055061]\n",
      "epoch: 218 loss: [0.09978841]\n",
      "epoch: 219 loss: [0.09903389]\n",
      "epoch: 220 loss: [0.09828697]\n",
      "epoch: 221 loss: [0.09754757]\n",
      "epoch: 222 loss: [0.0968156]\n",
      "epoch: 223 loss: [0.09609098]\n",
      "epoch: 224 loss: [0.09537364]\n",
      "epoch: 225 loss: [0.09466349]\n",
      "epoch: 226 loss: [0.09396045]\n",
      "epoch: 227 loss: [0.09326444]\n",
      "epoch: 228 loss: [0.09257539]\n",
      "epoch: 229 loss: [0.09189322]\n",
      "epoch: 230 loss: [0.09121786]\n",
      "epoch: 231 loss: [0.09054922]\n",
      "epoch: 232 loss: [0.08988725]\n",
      "epoch: 233 loss: [0.08923185]\n",
      "epoch: 234 loss: [0.08858297]\n",
      "epoch: 235 loss: [0.08794053]\n",
      "epoch: 236 loss: [0.08730446]\n",
      "epoch: 237 loss: [0.0866747]\n",
      "epoch: 238 loss: [0.08605116]\n",
      "epoch: 239 loss: [0.08543379]\n",
      "epoch: 240 loss: [0.08482252]\n",
      "epoch: 241 loss: [0.08421728]\n",
      "epoch: 242 loss: [0.083618]\n",
      "epoch: 243 loss: [0.08302463]\n",
      "epoch: 244 loss: [0.08243709]\n",
      "epoch: 245 loss: [0.08185533]\n",
      "epoch: 246 loss: [0.08127928]\n",
      "epoch: 247 loss: [0.08070888]\n",
      "epoch: 248 loss: [0.08014407]\n",
      "epoch: 249 loss: [0.07958478]\n",
      "epoch: 250 loss: [0.07903097]\n",
      "epoch: 251 loss: [0.07848257]\n",
      "epoch: 252 loss: [0.07793951]\n",
      "epoch: 253 loss: [0.07740176]\n",
      "epoch: 254 loss: [0.07686923]\n",
      "epoch: 255 loss: [0.07634189]\n",
      "epoch: 256 loss: [0.07581968]\n",
      "epoch: 257 loss: [0.07530254]\n",
      "epoch: 258 loss: [0.07479041]\n",
      "epoch: 259 loss: [0.07428324]\n",
      "epoch: 260 loss: [0.07378099]\n",
      "epoch: 261 loss: [0.07328359]\n",
      "epoch: 262 loss: [0.07279099]\n",
      "epoch: 263 loss: [0.07230315]\n",
      "epoch: 264 loss: [0.07182001]\n",
      "epoch: 265 loss: [0.07134152]\n",
      "epoch: 266 loss: [0.07086764]\n",
      "epoch: 267 loss: [0.07039831]\n",
      "epoch: 268 loss: [0.06993349]\n",
      "epoch: 269 loss: [0.06947312]\n",
      "epoch: 270 loss: [0.06901717]\n",
      "epoch: 271 loss: [0.06856558]\n",
      "epoch: 272 loss: [0.06811831]\n",
      "epoch: 273 loss: [0.0676753]\n",
      "epoch: 274 loss: [0.06723653]\n",
      "epoch: 275 loss: [0.06680194]\n",
      "epoch: 276 loss: [0.06637148]\n",
      "epoch: 277 loss: [0.06594512]\n",
      "epoch: 278 loss: [0.06552281]\n",
      "epoch: 279 loss: [0.06510451]\n",
      "epoch: 280 loss: [0.06469017]\n",
      "epoch: 281 loss: [0.06427976]\n",
      "epoch: 282 loss: [0.06387323]\n",
      "epoch: 283 loss: [0.06347055]\n",
      "epoch: 284 loss: [0.06307166]\n",
      "epoch: 285 loss: [0.06267654]\n",
      "epoch: 286 loss: [0.06228514]\n",
      "epoch: 287 loss: [0.06189743]\n",
      "epoch: 288 loss: [0.06151335]\n",
      "epoch: 289 loss: [0.06113289]\n",
      "epoch: 290 loss: [0.060756]\n",
      "epoch: 291 loss: [0.06038264]\n",
      "epoch: 292 loss: [0.06001277]\n",
      "epoch: 293 loss: [0.05964636]\n",
      "epoch: 294 loss: [0.05928338]\n",
      "epoch: 295 loss: [0.05892379]\n",
      "epoch: 296 loss: [0.05856754]\n",
      "epoch: 297 loss: [0.05821462]\n",
      "epoch: 298 loss: [0.05786497]\n",
      "epoch: 299 loss: [0.05751858]\n",
      "epoch: 300 loss: [0.0571754]\n",
      "epoch: 301 loss: [0.05683541]\n",
      "epoch: 302 loss: [0.05649856]\n",
      "epoch: 303 loss: [0.05616483]\n",
      "epoch: 304 loss: [0.05583419]\n",
      "epoch: 305 loss: [0.05550659]\n",
      "epoch: 306 loss: [0.05518202]\n",
      "epoch: 307 loss: [0.05486044]\n",
      "epoch: 308 loss: [0.05454182]\n",
      "epoch: 309 loss: [0.05422613]\n",
      "epoch: 310 loss: [0.05391334]\n",
      "epoch: 311 loss: [0.05360342]\n",
      "epoch: 312 loss: [0.05329634]\n",
      "epoch: 313 loss: [0.05299207]\n",
      "epoch: 314 loss: [0.05269058]\n",
      "epoch: 315 loss: [0.05239185]\n",
      "epoch: 316 loss: [0.05209584]\n",
      "epoch: 317 loss: [0.05180253]\n",
      "epoch: 318 loss: [0.0515119]\n",
      "epoch: 319 loss: [0.0512239]\n",
      "epoch: 320 loss: [0.05093852]\n",
      "epoch: 321 loss: [0.05065574]\n",
      "epoch: 322 loss: [0.05037551]\n",
      "epoch: 323 loss: [0.05009783]\n",
      "epoch: 324 loss: [0.04982265]\n",
      "epoch: 325 loss: [0.04954996]\n",
      "epoch: 326 loss: [0.04927974]\n",
      "epoch: 327 loss: [0.04901195]\n",
      "epoch: 328 loss: [0.04874657]\n",
      "epoch: 329 loss: [0.04848358]\n",
      "epoch: 330 loss: [0.04822296]\n",
      "epoch: 331 loss: [0.04796468]\n",
      "epoch: 332 loss: [0.04770871]\n",
      "epoch: 333 loss: [0.04745503]\n",
      "epoch: 334 loss: [0.04720363]\n",
      "epoch: 335 loss: [0.04695447]\n",
      "epoch: 336 loss: [0.04670754]\n",
      "epoch: 337 loss: [0.04646281]\n",
      "epoch: 338 loss: [0.04622026]\n",
      "epoch: 339 loss: [0.04597987]\n",
      "epoch: 340 loss: [0.04574162]\n",
      "epoch: 341 loss: [0.04550549]\n",
      "epoch: 342 loss: [0.04527145]\n",
      "epoch: 343 loss: [0.04503948]\n",
      "epoch: 344 loss: [0.04480957]\n",
      "epoch: 345 loss: [0.04458169]\n",
      "epoch: 346 loss: [0.04435582]\n",
      "epoch: 347 loss: [0.04413195]\n",
      "epoch: 348 loss: [0.04391005]\n",
      "epoch: 349 loss: [0.04369011]\n",
      "epoch: 350 loss: [0.0434721]\n",
      "epoch: 351 loss: [0.043256]\n",
      "epoch: 352 loss: [0.0430418]\n",
      "epoch: 353 loss: [0.04282948]\n",
      "epoch: 354 loss: [0.04261902]\n",
      "epoch: 355 loss: [0.0424104]\n",
      "epoch: 356 loss: [0.04220361]\n",
      "epoch: 357 loss: [0.04199862]\n",
      "epoch: 358 loss: [0.04179541]\n",
      "epoch: 359 loss: [0.04159398]\n",
      "epoch: 360 loss: [0.0413943]\n",
      "epoch: 361 loss: [0.04119636]\n",
      "epoch: 362 loss: [0.04100013]\n",
      "epoch: 363 loss: [0.04080561]\n",
      "epoch: 364 loss: [0.04061277]\n",
      "epoch: 365 loss: [0.0404216]\n",
      "epoch: 366 loss: [0.04023209]\n",
      "epoch: 367 loss: [0.04004421]\n",
      "epoch: 368 loss: [0.03985795]\n",
      "epoch: 369 loss: [0.0396733]\n",
      "epoch: 370 loss: [0.03949023]\n",
      "epoch: 371 loss: [0.03930874]\n",
      "epoch: 372 loss: [0.03912881]\n",
      "epoch: 373 loss: [0.03895043]\n",
      "epoch: 374 loss: [0.03877357]\n",
      "epoch: 375 loss: [0.03859823]\n",
      "epoch: 376 loss: [0.03842439]\n",
      "epoch: 377 loss: [0.03825203]\n",
      "epoch: 378 loss: [0.03808115]\n",
      "epoch: 379 loss: [0.03791172]\n",
      "epoch: 380 loss: [0.03774373]\n",
      "epoch: 381 loss: [0.03757718]\n",
      "epoch: 382 loss: [0.03741204]\n",
      "epoch: 383 loss: [0.0372483]\n",
      "epoch: 384 loss: [0.03708595]\n",
      "epoch: 385 loss: [0.03692497]\n",
      "epoch: 386 loss: [0.03676536]\n",
      "epoch: 387 loss: [0.0366071]\n",
      "epoch: 388 loss: [0.03645018]\n",
      "epoch: 389 loss: [0.03629457]\n",
      "epoch: 390 loss: [0.03614028]\n",
      "epoch: 391 loss: [0.03598729]\n",
      "epoch: 392 loss: [0.03583559]\n",
      "epoch: 393 loss: [0.03568516]\n",
      "epoch: 394 loss: [0.03553599]\n",
      "epoch: 395 loss: [0.03538808]\n",
      "epoch: 396 loss: [0.0352414]\n",
      "epoch: 397 loss: [0.03509595]\n",
      "epoch: 398 loss: [0.03495171]\n",
      "epoch: 399 loss: [0.03480868]\n",
      "epoch: 400 loss: [0.03466685]\n",
      "epoch: 401 loss: [0.03452619]\n",
      "epoch: 402 loss: [0.0343867]\n",
      "epoch: 403 loss: [0.03424838]\n",
      "epoch: 404 loss: [0.0341112]\n",
      "epoch: 405 loss: [0.03397516]\n",
      "epoch: 406 loss: [0.03384025]\n",
      "epoch: 407 loss: [0.03370646]\n",
      "epoch: 408 loss: [0.03357377]\n",
      "epoch: 409 loss: [0.03344218]\n",
      "epoch: 410 loss: [0.03331167]\n",
      "epoch: 411 loss: [0.03318225]\n",
      "epoch: 412 loss: [0.03305388]\n",
      "epoch: 413 loss: [0.03292658]\n",
      "epoch: 414 loss: [0.03280032]\n",
      "epoch: 415 loss: [0.03267509]\n",
      "epoch: 416 loss: [0.0325509]\n",
      "epoch: 417 loss: [0.03242772]\n",
      "epoch: 418 loss: [0.03230555]\n",
      "epoch: 419 loss: [0.03218438]\n",
      "epoch: 420 loss: [0.0320642]\n",
      "epoch: 421 loss: [0.031945]\n",
      "epoch: 422 loss: [0.03182677]\n",
      "epoch: 423 loss: [0.03170951]\n",
      "epoch: 424 loss: [0.0315932]\n",
      "epoch: 425 loss: [0.03147784]\n",
      "epoch: 426 loss: [0.03136341]\n",
      "epoch: 427 loss: [0.03124991]\n",
      "epoch: 428 loss: [0.03113733]\n",
      "epoch: 429 loss: [0.03102566]\n",
      "epoch: 430 loss: [0.0309149]\n",
      "epoch: 431 loss: [0.03080503]\n",
      "epoch: 432 loss: [0.03069604]\n",
      "epoch: 433 loss: [0.03058794]\n",
      "epoch: 434 loss: [0.03048071]\n",
      "epoch: 435 loss: [0.03037434]\n",
      "epoch: 436 loss: [0.03026882]\n",
      "epoch: 437 loss: [0.03016416]\n",
      "epoch: 438 loss: [0.03006033]\n",
      "epoch: 439 loss: [0.02995734]\n",
      "epoch: 440 loss: [0.02985517]\n",
      "epoch: 441 loss: [0.02975381]\n",
      "epoch: 442 loss: [0.02965327]\n",
      "epoch: 443 loss: [0.02955353]\n",
      "epoch: 444 loss: [0.02945459]\n",
      "epoch: 445 loss: [0.02935644]\n",
      "epoch: 446 loss: [0.02925906]\n",
      "epoch: 447 loss: [0.02916246]\n",
      "epoch: 448 loss: [0.02906663]\n",
      "epoch: 449 loss: [0.02897156]\n",
      "epoch: 450 loss: [0.02887725]\n",
      "epoch: 451 loss: [0.02878368]\n",
      "epoch: 452 loss: [0.02869085]\n",
      "epoch: 453 loss: [0.02859876]\n",
      "epoch: 454 loss: [0.02850739]\n",
      "epoch: 455 loss: [0.02841675]\n",
      "epoch: 456 loss: [0.02832682]\n",
      "epoch: 457 loss: [0.0282376]\n",
      "epoch: 458 loss: [0.02814908]\n",
      "epoch: 459 loss: [0.02806125]\n",
      "epoch: 460 loss: [0.02797412]\n",
      "epoch: 461 loss: [0.02788767]\n",
      "epoch: 462 loss: [0.0278019]\n",
      "epoch: 463 loss: [0.0277168]\n",
      "epoch: 464 loss: [0.02763237]\n",
      "epoch: 465 loss: [0.0275486]\n",
      "epoch: 466 loss: [0.02746548]\n",
      "epoch: 467 loss: [0.02738301]\n",
      "epoch: 468 loss: [0.02730118]\n",
      "epoch: 469 loss: [0.02722]\n",
      "epoch: 470 loss: [0.02713944]\n",
      "epoch: 471 loss: [0.02705951]\n",
      "epoch: 472 loss: [0.0269802]\n",
      "epoch: 473 loss: [0.02690151]\n",
      "epoch: 474 loss: [0.02682343]\n",
      "epoch: 475 loss: [0.02674595]\n",
      "epoch: 476 loss: [0.02666907]\n",
      "epoch: 477 loss: [0.02659279]\n",
      "epoch: 478 loss: [0.02651709]\n",
      "epoch: 479 loss: [0.02644198]\n",
      "epoch: 480 loss: [0.02636745]\n",
      "epoch: 481 loss: [0.0262935]\n",
      "epoch: 482 loss: [0.02622011]\n",
      "epoch: 483 loss: [0.02614729]\n",
      "epoch: 484 loss: [0.02607503]\n",
      "epoch: 485 loss: [0.02600332]\n",
      "epoch: 486 loss: [0.02593217]\n",
      "epoch: 487 loss: [0.02586156]\n",
      "epoch: 488 loss: [0.02579149]\n",
      "epoch: 489 loss: [0.02572195]\n",
      "epoch: 490 loss: [0.02565295]\n",
      "epoch: 491 loss: [0.02558448]\n",
      "epoch: 492 loss: [0.02551652]\n",
      "epoch: 493 loss: [0.02544909]\n",
      "epoch: 494 loss: [0.02538217]\n",
      "epoch: 495 loss: [0.02531576]\n",
      "epoch: 496 loss: [0.02524985]\n",
      "epoch: 497 loss: [0.02518445]\n",
      "epoch: 498 loss: [0.02511954]\n",
      "epoch: 499 loss: [0.02505512]\n",
      "epoch: 500 loss: [0.0249912]\n",
      "epoch: 501 loss: [0.02492775]\n",
      "epoch: 502 loss: [0.02486479]\n",
      "epoch: 503 loss: [0.02480231]\n",
      "epoch: 504 loss: [0.02474029]\n",
      "epoch: 505 loss: [0.02467874]\n",
      "epoch: 506 loss: [0.02461766]\n",
      "epoch: 507 loss: [0.02455704]\n",
      "epoch: 508 loss: [0.02449688]\n",
      "epoch: 509 loss: [0.02443716]\n",
      "epoch: 510 loss: [0.0243779]\n",
      "epoch: 511 loss: [0.02431908]\n",
      "epoch: 512 loss: [0.0242607]\n",
      "epoch: 513 loss: [0.02420277]\n",
      "epoch: 514 loss: [0.02414526]\n",
      "epoch: 515 loss: [0.02408819]\n",
      "epoch: 516 loss: [0.02403154]\n",
      "epoch: 517 loss: [0.02397531]\n",
      "epoch: 518 loss: [0.02391951]\n",
      "epoch: 519 loss: [0.02386412]\n",
      "epoch: 520 loss: [0.02380915]\n",
      "epoch: 521 loss: [0.02375458]\n",
      "epoch: 522 loss: [0.02370043]\n",
      "epoch: 523 loss: [0.02364667]\n",
      "epoch: 524 loss: [0.02359331]\n",
      "epoch: 525 loss: [0.02354035]\n",
      "epoch: 526 loss: [0.02348779]\n",
      "epoch: 527 loss: [0.02343561]\n",
      "epoch: 528 loss: [0.02338382]\n",
      "epoch: 529 loss: [0.02333241]\n",
      "epoch: 530 loss: [0.02328139]\n",
      "epoch: 531 loss: [0.02323074]\n",
      "epoch: 532 loss: [0.02318047]\n",
      "epoch: 533 loss: [0.02313056]\n",
      "epoch: 534 loss: [0.02308103]\n",
      "epoch: 535 loss: [0.02303186]\n",
      "epoch: 536 loss: [0.02298305]\n",
      "epoch: 537 loss: [0.0229346]\n",
      "epoch: 538 loss: [0.02288651]\n",
      "epoch: 539 loss: [0.02283877]\n",
      "epoch: 540 loss: [0.02279138]\n",
      "epoch: 541 loss: [0.02274434]\n",
      "epoch: 542 loss: [0.02269764]\n",
      "epoch: 543 loss: [0.02265129]\n",
      "epoch: 544 loss: [0.02260527]\n",
      "epoch: 545 loss: [0.0225596]\n",
      "epoch: 546 loss: [0.02251425]\n",
      "epoch: 547 loss: [0.02246924]\n",
      "epoch: 548 loss: [0.02242456]\n",
      "epoch: 549 loss: [0.0223802]\n",
      "epoch: 550 loss: [0.02233617]\n",
      "epoch: 551 loss: [0.02229245]\n",
      "epoch: 552 loss: [0.02224906]\n",
      "epoch: 553 loss: [0.02220598]\n",
      "epoch: 554 loss: [0.02216321]\n",
      "epoch: 555 loss: [0.02212075]\n",
      "epoch: 556 loss: [0.02207861]\n",
      "epoch: 557 loss: [0.02203676]\n",
      "epoch: 558 loss: [0.02199523]\n",
      "epoch: 559 loss: [0.02195399]\n",
      "epoch: 560 loss: [0.02191305]\n",
      "epoch: 561 loss: [0.02187241]\n",
      "epoch: 562 loss: [0.02183206]\n",
      "epoch: 563 loss: [0.021792]\n",
      "epoch: 564 loss: [0.02175223]\n",
      "epoch: 565 loss: [0.02171275]\n",
      "epoch: 566 loss: [0.02167355]\n",
      "epoch: 567 loss: [0.02163464]\n",
      "epoch: 568 loss: [0.021596]\n",
      "epoch: 569 loss: [0.02155764]\n",
      "epoch: 570 loss: [0.02151956]\n",
      "epoch: 571 loss: [0.02148176]\n",
      "epoch: 572 loss: [0.02144422]\n",
      "epoch: 573 loss: [0.02140695]\n",
      "epoch: 574 loss: [0.02136996]\n",
      "epoch: 575 loss: [0.02133322]\n",
      "epoch: 576 loss: [0.02129675]\n",
      "epoch: 577 loss: [0.02126054]\n",
      "epoch: 578 loss: [0.02122459]\n",
      "epoch: 579 loss: [0.0211889]\n",
      "epoch: 580 loss: [0.02115346]\n",
      "epoch: 581 loss: [0.02111827]\n",
      "epoch: 582 loss: [0.02108334]\n",
      "epoch: 583 loss: [0.02104865]\n",
      "epoch: 584 loss: [0.02101421]\n",
      "epoch: 585 loss: [0.02098002]\n",
      "epoch: 586 loss: [0.02094607]\n",
      "epoch: 587 loss: [0.02091236]\n",
      "epoch: 588 loss: [0.02087889]\n",
      "epoch: 589 loss: [0.02084566]\n",
      "epoch: 590 loss: [0.02081266]\n",
      "epoch: 591 loss: [0.0207799]\n",
      "epoch: 592 loss: [0.02074737]\n",
      "epoch: 593 loss: [0.02071507]\n",
      "epoch: 594 loss: [0.020683]\n",
      "epoch: 595 loss: [0.02065116]\n",
      "epoch: 596 loss: [0.02061954]\n",
      "epoch: 597 loss: [0.02058814]\n",
      "epoch: 598 loss: [0.02055697]\n",
      "epoch: 599 loss: [0.02052602]\n",
      "epoch: 600 loss: [0.02049529]\n",
      "epoch: 601 loss: [0.02046477]\n",
      "epoch: 602 loss: [0.02043446]\n",
      "epoch: 603 loss: [0.02040438]\n",
      "epoch: 604 loss: [0.0203745]\n",
      "epoch: 605 loss: [0.02034483]\n",
      "epoch: 606 loss: [0.02031537]\n",
      "epoch: 607 loss: [0.02028612]\n",
      "epoch: 608 loss: [0.02025708]\n",
      "epoch: 609 loss: [0.02022823]\n",
      "epoch: 610 loss: [0.02019959]\n",
      "epoch: 611 loss: [0.02017115]\n",
      "epoch: 612 loss: [0.02014291]\n",
      "epoch: 613 loss: [0.02011487]\n",
      "epoch: 614 loss: [0.02008703]\n",
      "epoch: 615 loss: [0.02005938]\n",
      "epoch: 616 loss: [0.02003192]\n",
      "epoch: 617 loss: [0.02000465]\n",
      "epoch: 618 loss: [0.01997758]\n",
      "epoch: 619 loss: [0.01995069]\n",
      "epoch: 620 loss: [0.01992399]\n",
      "epoch: 621 loss: [0.01989748]\n",
      "epoch: 622 loss: [0.01987115]\n",
      "epoch: 623 loss: [0.01984501]\n",
      "epoch: 624 loss: [0.01981905]\n",
      "epoch: 625 loss: [0.01979326]\n",
      "epoch: 626 loss: [0.01976766]\n",
      "epoch: 627 loss: [0.01974224]\n",
      "epoch: 628 loss: [0.01971699]\n",
      "epoch: 629 loss: [0.01969192]\n",
      "epoch: 630 loss: [0.01966702]\n",
      "epoch: 631 loss: [0.01964229]\n",
      "epoch: 632 loss: [0.01961774]\n",
      "epoch: 633 loss: [0.01959335]\n",
      "epoch: 634 loss: [0.01956914]\n",
      "epoch: 635 loss: [0.01954509]\n",
      "epoch: 636 loss: [0.01952121]\n",
      "epoch: 637 loss: [0.01949749]\n",
      "epoch: 638 loss: [0.01947394]\n",
      "epoch: 639 loss: [0.01945055]\n",
      "epoch: 640 loss: [0.01942732]\n",
      "epoch: 641 loss: [0.01940425]\n",
      "epoch: 642 loss: [0.01938134]\n",
      "epoch: 643 loss: [0.01935859]\n",
      "epoch: 644 loss: [0.019336]\n",
      "epoch: 645 loss: [0.01931356]\n",
      "epoch: 646 loss: [0.01929128]\n",
      "epoch: 647 loss: [0.01926915]\n",
      "epoch: 648 loss: [0.01924717]\n",
      "epoch: 649 loss: [0.01922534]\n",
      "epoch: 650 loss: [0.01920366]\n",
      "epoch: 651 loss: [0.01918213]\n",
      "epoch: 652 loss: [0.01916075]\n",
      "epoch: 653 loss: [0.01913952]\n",
      "epoch: 654 loss: [0.01911843]\n",
      "epoch: 655 loss: [0.01909749]\n",
      "epoch: 656 loss: [0.01907669]\n",
      "epoch: 657 loss: [0.01905603]\n",
      "epoch: 658 loss: [0.01903551]\n",
      "epoch: 659 loss: [0.01901514]\n",
      "epoch: 660 loss: [0.0189949]\n",
      "epoch: 661 loss: [0.0189748]\n",
      "epoch: 662 loss: [0.01895484]\n",
      "epoch: 663 loss: [0.01893502]\n",
      "epoch: 664 loss: [0.01891533]\n",
      "epoch: 665 loss: [0.01889577]\n",
      "epoch: 666 loss: [0.01887635]\n",
      "epoch: 667 loss: [0.01885706]\n",
      "epoch: 668 loss: [0.0188379]\n",
      "epoch: 669 loss: [0.01881888]\n",
      "epoch: 670 loss: [0.01879998]\n",
      "epoch: 671 loss: [0.01878121]\n",
      "epoch: 672 loss: [0.01876257]\n",
      "epoch: 673 loss: [0.01874405]\n",
      "epoch: 674 loss: [0.01872567]\n",
      "epoch: 675 loss: [0.0187074]\n",
      "epoch: 676 loss: [0.01868926]\n",
      "epoch: 677 loss: [0.01867125]\n",
      "epoch: 678 loss: [0.01865335]\n",
      "epoch: 679 loss: [0.01863558]\n",
      "epoch: 680 loss: [0.01861793]\n",
      "epoch: 681 loss: [0.0186004]\n",
      "epoch: 682 loss: [0.01858298]\n",
      "epoch: 683 loss: [0.01856569]\n",
      "epoch: 684 loss: [0.01854851]\n",
      "epoch: 685 loss: [0.01853145]\n",
      "epoch: 686 loss: [0.0185145]\n",
      "epoch: 687 loss: [0.01849767]\n",
      "epoch: 688 loss: [0.01848095]\n",
      "epoch: 689 loss: [0.01846434]\n",
      "epoch: 690 loss: [0.01844785]\n",
      "epoch: 691 loss: [0.01843147]\n",
      "epoch: 692 loss: [0.0184152]\n",
      "epoch: 693 loss: [0.01839903]\n",
      "epoch: 694 loss: [0.01838298]\n",
      "epoch: 695 loss: [0.01836704]\n",
      "epoch: 696 loss: [0.0183512]\n",
      "epoch: 697 loss: [0.01833547]\n",
      "epoch: 698 loss: [0.01831984]\n",
      "epoch: 699 loss: [0.01830433]\n",
      "epoch: 700 loss: [0.01828891]\n",
      "epoch: 701 loss: [0.0182736]\n",
      "epoch: 702 loss: [0.01825839]\n",
      "epoch: 703 loss: [0.01824329]\n",
      "epoch: 704 loss: [0.01822828]\n",
      "epoch: 705 loss: [0.01821338]\n",
      "epoch: 706 loss: [0.01819857]\n",
      "epoch: 707 loss: [0.01818387]\n",
      "epoch: 708 loss: [0.01816926]\n",
      "epoch: 709 loss: [0.01815476]\n",
      "epoch: 710 loss: [0.01814035]\n",
      "epoch: 711 loss: [0.01812603]\n",
      "epoch: 712 loss: [0.01811182]\n",
      "epoch: 713 loss: [0.01809769]\n",
      "epoch: 714 loss: [0.01808367]\n",
      "epoch: 715 loss: [0.01806973]\n",
      "epoch: 716 loss: [0.01805589]\n",
      "epoch: 717 loss: [0.01804214]\n",
      "epoch: 718 loss: [0.01802849]\n",
      "epoch: 719 loss: [0.01801492]\n",
      "epoch: 720 loss: [0.01800145]\n",
      "epoch: 721 loss: [0.01798806]\n",
      "epoch: 722 loss: [0.01797477]\n",
      "epoch: 723 loss: [0.01796156]\n",
      "epoch: 724 loss: [0.01794844]\n",
      "epoch: 725 loss: [0.01793541]\n",
      "epoch: 726 loss: [0.01792247]\n",
      "epoch: 727 loss: [0.01790961]\n",
      "epoch: 728 loss: [0.01789684]\n",
      "epoch: 729 loss: [0.01788415]\n",
      "epoch: 730 loss: [0.01787155]\n",
      "epoch: 731 loss: [0.01785903]\n",
      "epoch: 732 loss: [0.0178466]\n",
      "epoch: 733 loss: [0.01783424]\n",
      "epoch: 734 loss: [0.01782197]\n",
      "epoch: 735 loss: [0.01780979]\n",
      "epoch: 736 loss: [0.01779768]\n",
      "epoch: 737 loss: [0.01778565]\n",
      "epoch: 738 loss: [0.01777371]\n",
      "epoch: 739 loss: [0.01776184]\n",
      "epoch: 740 loss: [0.01775005]\n",
      "epoch: 741 loss: [0.01773834]\n",
      "epoch: 742 loss: [0.01772671]\n",
      "epoch: 743 loss: [0.01771515]\n",
      "epoch: 744 loss: [0.01770367]\n",
      "epoch: 745 loss: [0.01769227]\n",
      "epoch: 746 loss: [0.01768094]\n",
      "epoch: 747 loss: [0.01766969]\n",
      "epoch: 748 loss: [0.01765851]\n",
      "epoch: 749 loss: [0.01764741]\n",
      "epoch: 750 loss: [0.01763638]\n",
      "epoch: 751 loss: [0.01762542]\n",
      "epoch: 752 loss: [0.01761454]\n",
      "epoch: 753 loss: [0.01760372]\n",
      "epoch: 754 loss: [0.01759298]\n",
      "epoch: 755 loss: [0.01758231]\n",
      "epoch: 756 loss: [0.01757171]\n",
      "epoch: 757 loss: [0.01756118]\n",
      "epoch: 758 loss: [0.01755072]\n",
      "epoch: 759 loss: [0.01754033]\n",
      "epoch: 760 loss: [0.01753001]\n",
      "epoch: 761 loss: [0.01751975]\n",
      "epoch: 762 loss: [0.01750957]\n",
      "epoch: 763 loss: [0.01749945]\n",
      "epoch: 764 loss: [0.0174894]\n",
      "epoch: 765 loss: [0.01747941]\n",
      "epoch: 766 loss: [0.01746949]\n",
      "epoch: 767 loss: [0.01745963]\n",
      "epoch: 768 loss: [0.01744984]\n",
      "epoch: 769 loss: [0.01744012]\n",
      "epoch: 770 loss: [0.01743046]\n",
      "epoch: 771 loss: [0.01742086]\n",
      "epoch: 772 loss: [0.01741132]\n",
      "epoch: 773 loss: [0.01740185]\n",
      "epoch: 774 loss: [0.01739244]\n",
      "epoch: 775 loss: [0.0173831]\n",
      "epoch: 776 loss: [0.01737381]\n",
      "epoch: 777 loss: [0.01736458]\n",
      "epoch: 778 loss: [0.01735542]\n",
      "epoch: 779 loss: [0.01734631]\n",
      "epoch: 780 loss: [0.01733727]\n",
      "epoch: 781 loss: [0.01732828]\n",
      "epoch: 782 loss: [0.01731936]\n",
      "epoch: 783 loss: [0.01731049]\n",
      "epoch: 784 loss: [0.01730168]\n",
      "epoch: 785 loss: [0.01729293]\n",
      "epoch: 786 loss: [0.01728424]\n",
      "epoch: 787 loss: [0.0172756]\n",
      "epoch: 788 loss: [0.01726702]\n",
      "epoch: 789 loss: [0.01725849]\n",
      "epoch: 790 loss: [0.01725002]\n",
      "epoch: 791 loss: [0.01724161]\n",
      "epoch: 792 loss: [0.01723325]\n",
      "epoch: 793 loss: [0.01722495]\n",
      "epoch: 794 loss: [0.0172167]\n",
      "epoch: 795 loss: [0.01720851]\n",
      "epoch: 796 loss: [0.01720037]\n",
      "epoch: 797 loss: [0.01719228]\n",
      "epoch: 798 loss: [0.01718424]\n",
      "epoch: 799 loss: [0.01717626]\n",
      "epoch: 800 loss: [0.01716833]\n",
      "epoch: 801 loss: [0.01716045]\n",
      "epoch: 802 loss: [0.01715263]\n",
      "epoch: 803 loss: [0.01714485]\n",
      "epoch: 804 loss: [0.01713713]\n",
      "epoch: 805 loss: [0.01712945]\n",
      "epoch: 806 loss: [0.01712183]\n",
      "epoch: 807 loss: [0.01711425]\n",
      "epoch: 808 loss: [0.01710673]\n",
      "epoch: 809 loss: [0.01709925]\n",
      "epoch: 810 loss: [0.01709182]\n",
      "epoch: 811 loss: [0.01708445]\n",
      "epoch: 812 loss: [0.01707712]\n",
      "epoch: 813 loss: [0.01706983]\n",
      "epoch: 814 loss: [0.0170626]\n",
      "epoch: 815 loss: [0.01705541]\n",
      "epoch: 816 loss: [0.01704827]\n",
      "epoch: 817 loss: [0.01704117]\n",
      "epoch: 818 loss: [0.01703412]\n",
      "epoch: 819 loss: [0.01702712]\n",
      "epoch: 820 loss: [0.01702017]\n",
      "epoch: 821 loss: [0.01701325]\n",
      "epoch: 822 loss: [0.01700639]\n",
      "epoch: 823 loss: [0.01699957]\n",
      "epoch: 824 loss: [0.01699279]\n",
      "epoch: 825 loss: [0.01698605]\n",
      "epoch: 826 loss: [0.01697936]\n",
      "epoch: 827 loss: [0.01697272]\n",
      "epoch: 828 loss: [0.01696612]\n",
      "epoch: 829 loss: [0.01695956]\n",
      "epoch: 830 loss: [0.01695304]\n",
      "epoch: 831 loss: [0.01694656]\n",
      "epoch: 832 loss: [0.01694013]\n",
      "epoch: 833 loss: [0.01693374]\n",
      "epoch: 834 loss: [0.01692739]\n",
      "epoch: 835 loss: [0.01692108]\n",
      "epoch: 836 loss: [0.01691481]\n",
      "epoch: 837 loss: [0.01690859]\n",
      "epoch: 838 loss: [0.0169024]\n",
      "epoch: 839 loss: [0.01689626]\n",
      "epoch: 840 loss: [0.01689015]\n",
      "epoch: 841 loss: [0.01688408]\n",
      "epoch: 842 loss: [0.01687806]\n",
      "epoch: 843 loss: [0.01687207]\n",
      "epoch: 844 loss: [0.01686612]\n",
      "epoch: 845 loss: [0.01686021]\n",
      "epoch: 846 loss: [0.01685434]\n",
      "epoch: 847 loss: [0.0168485]\n",
      "epoch: 848 loss: [0.0168427]\n",
      "epoch: 849 loss: [0.01683695]\n",
      "epoch: 850 loss: [0.01683122]\n",
      "epoch: 851 loss: [0.01682554]\n",
      "epoch: 852 loss: [0.01681989]\n",
      "epoch: 853 loss: [0.01681428]\n",
      "epoch: 854 loss: [0.01680871]\n",
      "epoch: 855 loss: [0.01680317]\n",
      "epoch: 856 loss: [0.01679766]\n",
      "epoch: 857 loss: [0.0167922]\n",
      "epoch: 858 loss: [0.01678676]\n",
      "epoch: 859 loss: [0.01678137]\n",
      "epoch: 860 loss: [0.01677601]\n",
      "epoch: 861 loss: [0.01677068]\n",
      "epoch: 862 loss: [0.01676539]\n",
      "epoch: 863 loss: [0.01676013]\n",
      "epoch: 864 loss: [0.0167549]\n",
      "epoch: 865 loss: [0.01674971]\n",
      "epoch: 866 loss: [0.01674456]\n",
      "epoch: 867 loss: [0.01673943]\n",
      "epoch: 868 loss: [0.01673434]\n",
      "epoch: 869 loss: [0.01672928]\n",
      "epoch: 870 loss: [0.01672426]\n",
      "epoch: 871 loss: [0.01671926]\n",
      "epoch: 872 loss: [0.0167143]\n",
      "epoch: 873 loss: [0.01670938]\n",
      "epoch: 874 loss: [0.01670448]\n",
      "epoch: 875 loss: [0.01669961]\n",
      "epoch: 876 loss: [0.01669478]\n",
      "epoch: 877 loss: [0.01668998]\n",
      "epoch: 878 loss: [0.01668521]\n",
      "epoch: 879 loss: [0.01668047]\n",
      "epoch: 880 loss: [0.01667576]\n",
      "epoch: 881 loss: [0.01667108]\n",
      "epoch: 882 loss: [0.01666643]\n",
      "epoch: 883 loss: [0.01666181]\n",
      "epoch: 884 loss: [0.01665722]\n",
      "epoch: 885 loss: [0.01665266]\n",
      "epoch: 886 loss: [0.01664813]\n",
      "epoch: 887 loss: [0.01664362]\n",
      "epoch: 888 loss: [0.01663915]\n",
      "epoch: 889 loss: [0.01663471]\n",
      "epoch: 890 loss: [0.01663029]\n",
      "epoch: 891 loss: [0.01662591]\n",
      "epoch: 892 loss: [0.01662155]\n",
      "epoch: 893 loss: [0.01661722]\n",
      "epoch: 894 loss: [0.01661291]\n",
      "epoch: 895 loss: [0.01660864]\n",
      "epoch: 896 loss: [0.01660439]\n",
      "epoch: 897 loss: [0.01660017]\n",
      "epoch: 898 loss: [0.01659598]\n",
      "epoch: 899 loss: [0.01659181]\n",
      "epoch: 900 loss: [0.01658767]\n",
      "epoch: 901 loss: [0.01658356]\n",
      "epoch: 902 loss: [0.01657948]\n",
      "epoch: 903 loss: [0.01657542]\n",
      "epoch: 904 loss: [0.01657138]\n",
      "epoch: 905 loss: [0.01656737]\n",
      "epoch: 906 loss: [0.01656339]\n",
      "epoch: 907 loss: [0.01655944]\n",
      "epoch: 908 loss: [0.01655551]\n",
      "epoch: 909 loss: [0.0165516]\n",
      "epoch: 910 loss: [0.01654772]\n",
      "epoch: 911 loss: [0.01654386]\n",
      "epoch: 912 loss: [0.01654003]\n",
      "epoch: 913 loss: [0.01653623]\n",
      "epoch: 914 loss: [0.01653245]\n",
      "epoch: 915 loss: [0.01652869]\n",
      "epoch: 916 loss: [0.01652495]\n",
      "epoch: 917 loss: [0.01652125]\n",
      "epoch: 918 loss: [0.01651756]\n",
      "epoch: 919 loss: [0.0165139]\n",
      "epoch: 920 loss: [0.01651026]\n",
      "epoch: 921 loss: [0.01650664]\n",
      "epoch: 922 loss: [0.01650305]\n",
      "epoch: 923 loss: [0.01649948]\n",
      "epoch: 924 loss: [0.01649594]\n",
      "epoch: 925 loss: [0.01649241]\n",
      "epoch: 926 loss: [0.01648891]\n",
      "epoch: 927 loss: [0.01648544]\n",
      "epoch: 928 loss: [0.01648198]\n",
      "epoch: 929 loss: [0.01647855]\n",
      "epoch: 930 loss: [0.01647514]\n",
      "epoch: 931 loss: [0.01647175]\n",
      "epoch: 932 loss: [0.01646838]\n",
      "epoch: 933 loss: [0.01646503]\n",
      "epoch: 934 loss: [0.01646171]\n",
      "epoch: 935 loss: [0.0164584]\n",
      "epoch: 936 loss: [0.01645512]\n",
      "epoch: 937 loss: [0.01645186]\n",
      "epoch: 938 loss: [0.01644862]\n",
      "epoch: 939 loss: [0.0164454]\n",
      "epoch: 940 loss: [0.0164422]\n",
      "epoch: 941 loss: [0.01643902]\n",
      "epoch: 942 loss: [0.01643586]\n",
      "epoch: 943 loss: [0.01643272]\n",
      "epoch: 944 loss: [0.01642961]\n",
      "epoch: 945 loss: [0.01642651]\n",
      "epoch: 946 loss: [0.01642343]\n",
      "epoch: 947 loss: [0.01642037]\n",
      "epoch: 948 loss: [0.01641733]\n",
      "epoch: 949 loss: [0.01641431]\n",
      "epoch: 950 loss: [0.01641131]\n",
      "epoch: 951 loss: [0.01640833]\n",
      "epoch: 952 loss: [0.01640537]\n",
      "epoch: 953 loss: [0.01640243]\n",
      "epoch: 954 loss: [0.0163995]\n",
      "epoch: 955 loss: [0.0163966]\n",
      "epoch: 956 loss: [0.01639371]\n",
      "epoch: 957 loss: [0.01639084]\n",
      "epoch: 958 loss: [0.01638799]\n",
      "epoch: 959 loss: [0.01638516]\n",
      "epoch: 960 loss: [0.01638235]\n",
      "epoch: 961 loss: [0.01637955]\n",
      "epoch: 962 loss: [0.01637678]\n",
      "epoch: 963 loss: [0.01637402]\n",
      "epoch: 964 loss: [0.01637127]\n",
      "epoch: 965 loss: [0.01636855]\n",
      "epoch: 966 loss: [0.01636584]\n",
      "epoch: 967 loss: [0.01636315]\n",
      "epoch: 968 loss: [0.01636048]\n",
      "epoch: 969 loss: [0.01635782]\n",
      "epoch: 970 loss: [0.01635518]\n",
      "epoch: 971 loss: [0.01635256]\n",
      "epoch: 972 loss: [0.01634996]\n",
      "epoch: 973 loss: [0.01634737]\n",
      "epoch: 974 loss: [0.0163448]\n",
      "epoch: 975 loss: [0.01634224]\n",
      "epoch: 976 loss: [0.0163397]\n",
      "epoch: 977 loss: [0.01633718]\n",
      "epoch: 978 loss: [0.01633467]\n",
      "epoch: 979 loss: [0.01633218]\n",
      "epoch: 980 loss: [0.01632971]\n",
      "epoch: 981 loss: [0.01632725]\n",
      "epoch: 982 loss: [0.0163248]\n",
      "epoch: 983 loss: [0.01632237]\n",
      "epoch: 984 loss: [0.01631996]\n",
      "epoch: 985 loss: [0.01631757]\n",
      "epoch: 986 loss: [0.01631518]\n",
      "epoch: 987 loss: [0.01631282]\n",
      "epoch: 988 loss: [0.01631047]\n",
      "epoch: 989 loss: [0.01630813]\n",
      "epoch: 990 loss: [0.01630581]\n",
      "epoch: 991 loss: [0.0163035]\n",
      "epoch: 992 loss: [0.01630121]\n",
      "epoch: 993 loss: [0.01629893]\n",
      "epoch: 994 loss: [0.01629667]\n",
      "epoch: 995 loss: [0.01629442]\n",
      "epoch: 996 loss: [0.01629219]\n",
      "epoch: 997 loss: [0.01628997]\n",
      "epoch: 998 loss: [0.01628776]\n",
      "epoch: 999 loss: [0.01628557]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0.24858386288124149 err: [-0.24858386]\n",
      "[0. 1.] [1.] -> 0.7493469395704597 err: [0.25065306]\n",
      "[1. 0.] [1.] -> 0.7468098764553397 err: [0.25319012]\n",
      "[1. 1.] [1.] -> 1.247572953144558 err: [-0.24757295]\n"
     ]
    }
   ],
   "source": [
    "# 生の出力（活性化なしで）\n",
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [1.] -> 1 err: [0.]\n",
      "[1. 0.] [1.] -> 1 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "# ステップ関数を適用した後：\n",
    "print_results(model, step_activation, x_list, y_true)"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
